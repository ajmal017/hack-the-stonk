{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_datareader import data as pdr \n",
    "from datetime import date\n",
    "import yfinance as yf \n",
    "yf.pdr_override()\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "import quandl\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_sp = '^GSPC'\n",
    "ticker_gold = 'GC=F'\n",
    "ticker_oil = 'CL=F'\n",
    "ticker_dax = '^GDAXI'\n",
    "ticker_nikkei = '^N225'\n",
    "ticker_ftse = '^FTSE'\n",
    "ticker_shanghai = '000001.SS'\n",
    "\n",
    "auth_tok = \"Nv1rJgRR7u88iz_dg7Y6\"\n",
    "\n",
    "end_date = \"2020-09-1\"\n",
    "start_date = \"1970-01-02\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGOLDData ():\n",
    "    # Contains only price from 1975 onwards\n",
    "    data = quandl.get(\"CHRIS/CME_GC1\", trim_start = start_date, trim_end = end_date, authtoken=auth_tok)\n",
    "    data = data[['Last']]\n",
    "    data.columns = [\"GOLD Adj Close\"]\n",
    "    return data\n",
    "\n",
    "def getSPData():\n",
    "    # Contains price from 1970 onwards\n",
    "    data = pdr.get_data_yahoo(ticker_sp, start=start_date, end=end_date)\n",
    "    data = data[data.columns[4:5]] \n",
    "    data.columns = [\"SP500 Adj Close\"]\n",
    "    return data\n",
    "\n",
    "def getDAXData():\n",
    "    # Contains price from 1988 onwards\n",
    "    data = pdr.get_data_yahoo(ticker_dax, start=start_date, end=end_date)\n",
    "    data = data[data.columns[4:5]]\n",
    "    data.columns = [\"DAX Adj Close\"]\n",
    "    return data\n",
    "\n",
    "\n",
    "def getOILData():\n",
    "    # Contains only price FROM 1984 onwards\n",
    "    data = quandl.get(\"CHRIS/CME_CL1\", trim_start = start_date, trim_end = end_date, authtoken=auth_tok)\n",
    "    data = data[[\"Last\"]]\n",
    "    data.columns=[\"OIL Adj Close\"]\n",
    "    return data\n",
    "\n",
    "\n",
    "def getNIKKEIData():\n",
    "    # Contains only price from 1970 onwards\n",
    "    data = pdr.get_data_yahoo(ticker_nikkei, start=start_date, end=end_date)\n",
    "    data = data[data.columns[4:5]] \n",
    "    data.columns = [\"NIKKEI Adj Close\"]\n",
    "    return data\n",
    "\n",
    "\n",
    "def getFTSEData():\n",
    "    # Contains price from 1984 onwards\n",
    "    data = pdr.get_data_yahoo(ticker_ftse, start=start_date, end=end_date)\n",
    "    data = data[data.columns[4:5]] \n",
    "    data.columns = [\"FTSE Adj Close\"]\n",
    "    return data\n",
    "\n",
    "def getSHANGHAIData():\n",
    "    # Contains only price from 1997\n",
    "    data = pdr.get_data_yahoo(ticker_shanghai, start=start_date, end=end_date)\n",
    "    data = data[data.columns[4:5]] \n",
    "    data.columns = [\"SHANGHAI Adj Close\"]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineData():\n",
    "    allData = [getSPData(), getDAXData(), getFTSEData(), getGOLDData(), getOILData()]\n",
    "    mergedData = pd.concat(allData, axis = 1)\n",
    "    cleanData = mergedData.dropna()\n",
    "    return cleanData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "size of training data: 6723\n",
      "size of testing data: 1187\n"
     ]
    }
   ],
   "source": [
    "data = combineData()\n",
    "\n",
    "FUTURE_TO_PREDICT = 1 # Number of days into the future we want to predict\n",
    "\n",
    "data['Future'] = data['SP500 Adj Close'].shift(-FUTURE_TO_PREDICT)\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "def buy_or_sell (current, future):\n",
    "    if (future > current):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "data['Target'] = list(map(buy_or_sell, data['SP500 Adj Close'], data['Future']))\n",
    "\n",
    "NUMBER_OF_DATA_POINTS = len(data)\n",
    "SIZE_TRAINING = int(NUMBER_OF_DATA_POINTS * 0.85)\n",
    "SIZE_TESTING  = NUMBER_OF_DATA_POINTS - SIZE_TRAINING\n",
    "print(\"size of training data: {}\".format(SIZE_TRAINING))\n",
    "print(\"size of testing data: {}\".format(SIZE_TESTING))\n",
    "\n",
    "data_training = data[:SIZE_TRAINING]\n",
    "data_testing  = data[SIZE_TRAINING:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            SP500 Adj Close  DAX Adj Close  FTSE Adj Close  GOLD Adj Close  \\\n",
      "Date                                                                         \n",
      "1987-12-30       247.860001    1005.190002     1759.800049           485.5   \n",
      "1988-01-04       255.940002     956.489990     1713.900024           480.5   \n",
      "1988-01-05       258.630005     996.099976     1789.599976           483.2   \n",
      "1988-01-06       258.890015    1006.010010     1787.099976           485.3   \n",
      "1988-01-07       261.070007    1014.469971     1787.199951           483.1   \n",
      "...                     ...            ...             ...             ...   \n",
      "2015-08-25      1867.609985   10128.120117     6081.299805          1139.7   \n",
      "2015-08-26      1940.510010    9997.429688     5979.200195          1124.5   \n",
      "2015-08-27      1987.660034   10315.620117     6192.000000          1123.4   \n",
      "2015-08-28      1988.869995   10298.530273     6247.899902          1132.8   \n",
      "2015-09-01      1913.849976   10015.570312     6058.500000          1139.0   \n",
      "\n",
      "            OIL Adj Close       Future  Target  \n",
      "Date                                            \n",
      "1987-12-30          16.89   255.940002       1  \n",
      "1988-01-04          17.69   258.630005       1  \n",
      "1988-01-05          17.85   258.890015       1  \n",
      "1988-01-06          17.82   261.070007       1  \n",
      "1988-01-07          17.39   243.399994       0  \n",
      "...                   ...          ...     ...  \n",
      "2015-08-25          39.64  1940.510010       1  \n",
      "2015-08-26          38.88  1987.660034       1  \n",
      "2015-08-27          42.78  1988.869995       1  \n",
      "2015-08-28          45.33  1913.849976       0  \n",
      "2015-09-01          44.19  1948.859985       1  \n",
      "\n",
      "[6723 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    data.drop(\"Future\", axis = 1, inplace = True)  # Drop the future column so that the NN doesn't have access to the future\n",
    "    \n",
    "    for column in data.columns:  # Normalize the columns\n",
    "        if column != \"Target\":   # We only want to normalize the other columns\n",
    "            data[column] = data[column].pct_change()   # Normalization by percent change\n",
    "            #data.dropna(inplace = True)\n",
    "            #data[column] = preprocessing.scale(data[column].values)    # First testing without scaling\n",
    "    \n",
    "    data.dropna(inplace = True)\n",
    "\n",
    "    sequential_data = []\n",
    "    sequence_length = 15 # Number of days into the past we are using to make a prediction\n",
    "    \n",
    "    for day in range (len(data)-sequence_length+1):\n",
    "        \n",
    "        sequence = []\n",
    "        \n",
    "        for future_day in range (sequence_length):\n",
    "            sequence.append(data.iloc[day + future_day][:-1])\n",
    "        \n",
    "        buy_or_sell = data.iloc[day + sequence_length - 1][-1]\n",
    "        \n",
    "        sequential_data.append([sequence, buy_or_sell])\n",
    "    \n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    buy_sequences  = []\n",
    "    sell_sequences = []\n",
    "    \n",
    "    for sequence, target in sequential_data:\n",
    "        if target == 1:\n",
    "            buy_sequences.append([sequence, target])\n",
    "        elif target == 0:\n",
    "            sell_sequences.append([sequence, target])\n",
    "            \n",
    "    print(\"{} buys\".format(len(buy_sequences)))\n",
    "    print(\"{} sells\".format(len(sell_sequences)))\n",
    "    \n",
    "    random.shuffle(buy_sequences)\n",
    "    random.shuffle(sell_sequences)\n",
    "    \n",
    "    max_size = min(len(buy_sequences), len(sell_sequences))\n",
    "    \n",
    "    print(\"reduced to {} buys and sells\".format(max_size))\n",
    "    \n",
    "    buy_sequences  = buy_sequences[:max_size]\n",
    "    sell_sequences = sell_sequences[:max_size]\n",
    "    \n",
    "    sequential_data = buy_sequences + sell_sequences\n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for sequence, target in sequential_data:\n",
    "        x.append(sequence)\n",
    "        y.append(target)\n",
    "        \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3599 buys\n",
      "3109 sells\n",
      "reduced to 3109 buys and sells\n",
      "653 buys\n",
      "519 sells\n",
      "reduced to 519 buys and sells\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = process_data(data_training)\n",
    "x_test, y_test   = process_data(data_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "LSTM_REPRESENTATION = 16\n",
    "DENSE_REPRESENTATION = 16\n",
    "\n",
    "\n",
    "model.add(LSTM(LSTM_REPRESENTATION, activation = 'relu', input_shape = (x_train.shape[1:]), return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(LSTM_REPRESENTATION, return_sequences = True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(LSTM_REPRESENTATION))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(DENSE_REPRESENTATION, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay = 1e-6)\n",
    "\n",
    "model.compile(\n",
    "    loss = 'sparse_categorical_crossentropy',\n",
    "    optimizer = opt,\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs\\LSTM-{}-DENSE-{}\".format(LSTM_REPRESENTATION, DENSE_REPRESENTATION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6218 samples, validate on 1038 samples\n",
      "Epoch 1/50\n",
      "  32/6218 [..............................] - ETA: 12:13 - loss: 1.0699 - accuracy: 0.4688WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.136272). Check your callbacks.\n",
      "6218/6218 [==============================] - 8s 1ms/sample - loss: 0.7928 - accuracy: 0.4928 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "6218/6218 [==============================] - 3s 414us/sample - loss: 0.7240 - accuracy: 0.4968 - val_loss: 0.6931 - val_accuracy: 0.5019\n",
      "Epoch 3/50\n",
      "6218/6218 [==============================] - 3s 416us/sample - loss: 0.7082 - accuracy: 0.4984 - val_loss: 0.6966 - val_accuracy: 0.5039\n",
      "Epoch 4/50\n",
      "6218/6218 [==============================] - 3s 423us/sample - loss: 0.7019 - accuracy: 0.5053 - val_loss: 0.6954 - val_accuracy: 0.4904\n",
      "Epoch 5/50\n",
      "6218/6218 [==============================] - 3s 411us/sample - loss: 0.6993 - accuracy: 0.5103 - val_loss: 0.6966 - val_accuracy: 0.5000\n",
      "Epoch 6/50\n",
      "6218/6218 [==============================] - 3s 426us/sample - loss: 0.7004 - accuracy: 0.4867 - val_loss: 0.6933 - val_accuracy: 0.4952\n",
      "Epoch 7/50\n",
      "6218/6218 [==============================] - 3s 422us/sample - loss: 0.6942 - accuracy: 0.5040 - val_loss: 0.6936 - val_accuracy: 0.4961\n",
      "Epoch 8/50\n",
      "6218/6218 [==============================] - 3s 466us/sample - loss: 0.6965 - accuracy: 0.4971 - val_loss: 0.6942 - val_accuracy: 0.4904\n",
      "Epoch 9/50\n",
      "6218/6218 [==============================] - 3s 467us/sample - loss: 0.6950 - accuracy: 0.4931 - val_loss: 0.6963 - val_accuracy: 0.4981\n",
      "Epoch 10/50\n",
      "6218/6218 [==============================] - 3s 429us/sample - loss: 0.6938 - accuracy: 0.5084 - val_loss: 0.6933 - val_accuracy: 0.5058\n",
      "Epoch 11/50\n",
      "6218/6218 [==============================] - 3s 441us/sample - loss: 0.6952 - accuracy: 0.4994 - val_loss: 0.6940 - val_accuracy: 0.4778\n",
      "Epoch 12/50\n",
      "6218/6218 [==============================] - 3s 443us/sample - loss: 0.6943 - accuracy: 0.5047 - val_loss: 0.6939 - val_accuracy: 0.5029\n",
      "Epoch 13/50\n",
      "6218/6218 [==============================] - 3s 434us/sample - loss: 0.6944 - accuracy: 0.4945 - val_loss: 0.6933 - val_accuracy: 0.5048\n",
      "Epoch 14/50\n",
      "6218/6218 [==============================] - 3s 441us/sample - loss: 0.6937 - accuracy: 0.5072 - val_loss: 0.6938 - val_accuracy: 0.4836\n",
      "Epoch 15/50\n",
      "6218/6218 [==============================] - 3s 440us/sample - loss: 0.6937 - accuracy: 0.5026 - val_loss: 0.6938 - val_accuracy: 0.4894\n",
      "Epoch 16/50\n",
      "6218/6218 [==============================] - 3s 449us/sample - loss: 0.6939 - accuracy: 0.4998 - val_loss: 0.6938 - val_accuracy: 0.5048\n",
      "Epoch 17/50\n",
      "6218/6218 [==============================] - 3s 447us/sample - loss: 0.6939 - accuracy: 0.5047 - val_loss: 0.6933 - val_accuracy: 0.4971\n",
      "Epoch 18/50\n",
      "6218/6218 [==============================] - 3s 454us/sample - loss: 0.6939 - accuracy: 0.5006 - val_loss: 0.6938 - val_accuracy: 0.5029\n",
      "Epoch 19/50\n",
      "6218/6218 [==============================] - 3s 457us/sample - loss: 0.6938 - accuracy: 0.5043 - val_loss: 0.6936 - val_accuracy: 0.4740\n",
      "Epoch 20/50\n",
      "6218/6218 [==============================] - 3s 452us/sample - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6934 - val_accuracy: 0.4750\n",
      "Epoch 21/50\n",
      "6218/6218 [==============================] - 3s 451us/sample - loss: 0.6935 - accuracy: 0.5055 - val_loss: 0.6935 - val_accuracy: 0.4933\n",
      "Epoch 22/50\n",
      "6218/6218 [==============================] - 3s 456us/sample - loss: 0.6931 - accuracy: 0.5093 - val_loss: 0.6933 - val_accuracy: 0.4913\n",
      "Epoch 23/50\n",
      "6218/6218 [==============================] - 3s 450us/sample - loss: 0.6935 - accuracy: 0.5010 - val_loss: 0.6933 - val_accuracy: 0.4971\n",
      "Epoch 24/50\n",
      "6218/6218 [==============================] - 3s 455us/sample - loss: 0.6935 - accuracy: 0.5026 - val_loss: 0.6935 - val_accuracy: 0.4971\n",
      "Epoch 25/50\n",
      "6218/6218 [==============================] - 3s 449us/sample - loss: 0.6935 - accuracy: 0.5031 - val_loss: 0.6933 - val_accuracy: 0.5010\n",
      "Epoch 26/50\n",
      "6218/6218 [==============================] - 3s 450us/sample - loss: 0.6931 - accuracy: 0.5117 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 27/50\n",
      "6218/6218 [==============================] - 3s 456us/sample - loss: 0.6934 - accuracy: 0.4994 - val_loss: 0.6938 - val_accuracy: 0.4798\n",
      "Epoch 28/50\n",
      "6218/6218 [==============================] - 3s 454us/sample - loss: 0.6932 - accuracy: 0.5130 - val_loss: 0.6943 - val_accuracy: 0.4942\n",
      "Epoch 29/50\n",
      "6218/6218 [==============================] - 3s 450us/sample - loss: 0.6933 - accuracy: 0.5103 - val_loss: 0.6942 - val_accuracy: 0.4971\n",
      "Epoch 30/50\n",
      "6218/6218 [==============================] - 3s 459us/sample - loss: 0.6931 - accuracy: 0.5058 - val_loss: 0.6944 - val_accuracy: 0.4971\n",
      "Epoch 31/50\n",
      "6218/6218 [==============================] - 3s 461us/sample - loss: 0.6929 - accuracy: 0.5111 - val_loss: 0.6940 - val_accuracy: 0.4778\n",
      "Epoch 32/50\n",
      "6218/6218 [==============================] - 3s 460us/sample - loss: 0.6933 - accuracy: 0.5121 - val_loss: 0.6985 - val_accuracy: 0.5000\n",
      "Epoch 33/50\n",
      "6218/6218 [==============================] - 3s 453us/sample - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6929 - val_accuracy: 0.5048\n",
      "Epoch 34/50\n",
      "6218/6218 [==============================] - 3s 485us/sample - loss: 0.6933 - accuracy: 0.5066 - val_loss: 0.6931 - val_accuracy: 0.4990\n",
      "Epoch 35/50\n",
      "6218/6218 [==============================] - 3s 467us/sample - loss: 0.6933 - accuracy: 0.5018 - val_loss: 0.6943 - val_accuracy: 0.5000\n",
      "Epoch 36/50\n",
      "6218/6218 [==============================] - 3s 476us/sample - loss: 0.6929 - accuracy: 0.5109 - val_loss: 0.6933 - val_accuracy: 0.5029\n",
      "Epoch 37/50\n",
      "6218/6218 [==============================] - 3s 463us/sample - loss: 0.6921 - accuracy: 0.5121 - val_loss: 0.6947 - val_accuracy: 0.5029\n",
      "Epoch 38/50\n",
      "6218/6218 [==============================] - 3s 468us/sample - loss: 0.6927 - accuracy: 0.5161 - val_loss: 0.6935 - val_accuracy: 0.5039\n",
      "Epoch 39/50\n",
      "6218/6218 [==============================] - 3s 463us/sample - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6947 - val_accuracy: 0.4894\n",
      "Epoch 40/50\n",
      "6218/6218 [==============================] - 3s 455us/sample - loss: 0.6931 - accuracy: 0.5055 - val_loss: 0.6943 - val_accuracy: 0.4750\n",
      "Epoch 41/50\n",
      "6218/6218 [==============================] - 3s 482us/sample - loss: 0.6929 - accuracy: 0.5146 - val_loss: 0.6938 - val_accuracy: 0.4942\n",
      "Epoch 42/50\n",
      "6218/6218 [==============================] - 3s 464us/sample - loss: 0.6936 - accuracy: 0.5005 - val_loss: 0.6943 - val_accuracy: 0.5010\n",
      "Epoch 43/50\n",
      "6218/6218 [==============================] - 3s 461us/sample - loss: 0.6929 - accuracy: 0.5032 - val_loss: 0.6939 - val_accuracy: 0.4990\n",
      "Epoch 44/50\n",
      "6218/6218 [==============================] - 3s 467us/sample - loss: 0.6929 - accuracy: 0.5103 - val_loss: 0.6931 - val_accuracy: 0.5010\n",
      "Epoch 45/50\n",
      "6218/6218 [==============================] - 3s 473us/sample - loss: 0.6936 - accuracy: 0.4923 - val_loss: 0.6933 - val_accuracy: 0.4817\n",
      "Epoch 46/50\n",
      "6218/6218 [==============================] - 3s 458us/sample - loss: 0.6932 - accuracy: 0.4977 - val_loss: 0.6941 - val_accuracy: 0.4942\n",
      "Epoch 47/50\n",
      "6218/6218 [==============================] - 3s 469us/sample - loss: 0.6928 - accuracy: 0.5116 - val_loss: 0.6944 - val_accuracy: 0.4981\n",
      "Epoch 48/50\n",
      "6218/6218 [==============================] - 3s 469us/sample - loss: 0.6923 - accuracy: 0.5142 - val_loss: 0.6943 - val_accuracy: 0.4865\n",
      "Epoch 49/50\n",
      "6218/6218 [==============================] - 3s 459us/sample - loss: 0.6928 - accuracy: 0.5196 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 50/50\n",
      "6218/6218 [==============================] - 3s 463us/sample - loss: 0.6934 - accuracy: 0.5018 - val_loss: 0.6977 - val_accuracy: 0.4990\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size = 32,\n",
    "    epochs = 50,\n",
    "    validation_data = (x_test, y_test),\n",
    "    callbacks=[tensorboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "plt.plot(accuracy, label = \"Train Accuracy\")\n",
    "plt.plot(val_accuracy, label = \"Test Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
