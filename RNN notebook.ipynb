{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas_datareader\\compat\\__init__.py:7: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  from pandas.util.testing import assert_frame_equal\n"
     ]
    }
   ],
   "source": [
    "from pandas_datareader import data as pdr \n",
    "from datetime import date\n",
    "import yfinance as yf \n",
    "yf.pdr_override()\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "import quandl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_sp = '^GSPC'\n",
    "ticker_gold = 'GC=F'\n",
    "ticker_oil = 'CL=F'\n",
    "ticker_dax = '^GDAXI'\n",
    "ticker_nikkei = '^N225'\n",
    "ticker_ftse = '^FTSE'\n",
    "ticker_shanghai = '000001.SS'\n",
    "\n",
    "auth_tok = \"Nv1rJgRR7u88iz_dg7Y6\"\n",
    "\n",
    "end_date = \"2020-05-1\"\n",
    "start_date = \"2000-01-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGOLDData ():\n",
    "    # Contains price and volume\n",
    "    data = quandl.get(\"CHRIS/CME_GC1\", trim_start = start_date, trim_end = end_date, authtoken=auth_tok)\n",
    "    data = data[['Last']]\n",
    "    data.columns = [\"GOLD Adj Close\"]\n",
    "    return data\n",
    "\n",
    "def getSPData():\n",
    "    # Contains price and volume\n",
    "    data = pdr.get_data_yahoo(ticker_sp, start=start_date, end=end_date)\n",
    "    data = data[data.columns[4:6]] \n",
    "    data.columns = [\"SP500 Adj Close\",  \"SP500 Volume\"]\n",
    "    return data\n",
    "\n",
    "def getDAXData():\n",
    "    # Contains price and volume\n",
    "    data = pdr.get_data_yahoo(ticker_dax, start=start_date, end=end_date)\n",
    "    data = data[data.columns[4:6]]\n",
    "    data.columns = [\"DAX Adj Close\",  \"DAX Volume\"]\n",
    "    return data\n",
    "\n",
    "\n",
    "def getOILData():\n",
    "    # Contains price and volume\n",
    "    data = quandl.get(\"CHRIS/CME_CL1\", trim_start = start_date, trim_end = end_date, authtoken=auth_tok)\n",
    "    data = data[[\"Last\"]]\n",
    "    data.columns=[\"OIL Adj Close\"]\n",
    "    return data\n",
    "\n",
    "\n",
    "def getNIKKEIData():\n",
    "    # Contains only price\n",
    "    data = pdr.get_data_yahoo(ticker_nikkei, start=start_date, end=end_date)\n",
    "    data = data[data.columns[4:5]] \n",
    "    data.columns = [\"NIKKEI Adj Close\"]\n",
    "    return data\n",
    "\n",
    "\n",
    "def getFTSEData():\n",
    "    # Contains price and volume\n",
    "    data = pdr.get_data_yahoo(ticker_ftse, start=start_date, end=end_date)\n",
    "    data = data[data.columns[4:6]] \n",
    "    data.columns = [\"FTSE Adj Close\",  \"FTSE Volume\"]\n",
    "    return data\n",
    "\n",
    "def getSHANGHAIData():\n",
    "    # Contains only price\n",
    "    data = pdr.get_data_yahoo(ticker_shanghai, start=start_date, end=end_date)\n",
    "    data = data[data.columns[4:5]] \n",
    "    data.columns = [\"SHANGHAI Adj Close\"]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkData (data):\n",
    "    counter = 0\n",
    "    for index, row in data.iterrows():\n",
    "        for item in row:\n",
    "            if (math.isnan(item)):\n",
    "                counter += 1\n",
    "                break\n",
    "    return counter\n",
    "\n",
    "def normalizeData(data):\n",
    "    for column in data:\n",
    "        maxValue = max(data[column])\n",
    "        data[column] = data[column] / maxValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineData():\n",
    "    #allData = [getSPData(), getGOLDData(), getDAXData(), getOILData(), getNIKKEIData(), getSHANGHAIData(), getFTSEData()]\n",
    "    #allData = [getSPData()]\n",
    "    allData = [getSPData(), getDAXData(), getFTSEData(), getGOLDData(), getOILData()]\n",
    "    mergedData = pd.concat(allData, axis = 1)\n",
    "    print(\"REMOVED: {} DATA POINTS\".format(checkData(mergedData)))\n",
    "    cleanData = mergedData.dropna()\n",
    "    #normalizeData(cleanData)\n",
    "    return cleanData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "REMOVED: 339 DATA POINTS\n",
      "            SP500 Adj Close  SP500 Volume  DAX Adj Close   DAX Volume  \\\n",
      "Date                                                                    \n",
      "2000-01-04      1399.420044  1.009000e+09    6586.950195   46678400.0   \n",
      "2000-01-05      1402.109985  1.085500e+09    6502.069824   52682800.0   \n",
      "2000-01-06      1403.449951  1.092300e+09    6474.919922   41180600.0   \n",
      "2000-01-07      1441.469971  1.225200e+09    6780.959961   56058900.0   \n",
      "2000-01-10      1457.599976  1.064800e+09    6925.520020   42006200.0   \n",
      "...                     ...           ...            ...          ...   \n",
      "2020-04-24      2836.739990  5.374480e+09   10336.089844  120035000.0   \n",
      "2020-04-27      2878.479980  5.194260e+09   10659.990234  122476600.0   \n",
      "2020-04-28      2863.389893  5.672880e+09   10795.629883  146549900.0   \n",
      "2020-04-29      2939.510010  6.620140e+09   11107.740234  145350700.0   \n",
      "2020-04-30      2912.429932  6.523120e+09   10861.639648  162733400.0   \n",
      "\n",
      "            FTSE Adj Close   FTSE Volume  GOLD Adj Close  OIL Adj Close  \n",
      "Date                                                                     \n",
      "2000-01-04     6665.899902  6.334490e+08           283.7          25.55  \n",
      "2000-01-05     6535.899902  6.702340e+08           282.1          24.91  \n",
      "2000-01-06     6447.200195  7.855320e+08           282.4          24.78  \n",
      "2000-01-07     6504.799805  8.883060e+08           282.9          24.22  \n",
      "2000-01-10     6607.700195  7.354550e+08           282.7          24.67  \n",
      "...                    ...           ...             ...            ...  \n",
      "2020-04-24     5752.200195  8.088216e+08          1714.9          17.18  \n",
      "2020-04-27     5846.799805  7.922464e+08          1716.7          12.92  \n",
      "2020-04-28     5958.500000  1.196851e+09          1709.9          13.27  \n",
      "2020-04-29     6115.299805  1.288097e+09          1705.1          15.35  \n",
      "2020-04-30     5901.200195  1.933349e+09          1688.1          19.09  \n",
      "\n",
      "[4913 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "data = combineData()\n",
    "\n",
    "print(data)\n",
    "\n",
    "day_counter = 0\n",
    "\n",
    "# Size of prediction\n",
    "prediction_size = 1\n",
    "\n",
    "# Number of days in the past\n",
    "historical_size = 15\n",
    "\n",
    "# Size of available data.\n",
    "data_size = len(data) - prediction_size - historical_size\n",
    "\n",
    "# Size of data alocated to training\n",
    "training_size = int(0.85*data_size)\n",
    "\n",
    "# Size of data alocated to testing\n",
    "testing_size = data_size - training_size\n",
    "\n",
    "# Number of data elements in a single day\n",
    "daily_data_size = 8 \n",
    "\n",
    "x_train = np.zeros((training_size, historical_size, daily_data_size))\n",
    "y_train = np.zeros((training_size))\n",
    "x_test  = np.zeros((testing_size, historical_size, daily_data_size))\n",
    "y_test  = np.zeros((testing_size))\n",
    "\n",
    "sanity_train = np.zeros((training_size, 3))\n",
    "sanity_test = np.zeros((testing_size, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% completed\n"
     ]
    }
   ],
   "source": [
    "day_counter = 0\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    \n",
    "    if (day_counter == data_size):\n",
    "        print(\"100% completed\")\n",
    "        break\n",
    "\n",
    "    if (day_counter < len(x_train)):\n",
    "        \n",
    "        \n",
    "        for i in range (historical_size):\n",
    "            for j in range (daily_data_size):\n",
    "                x_train[day_counter][i][j] = data.iloc[day_counter + i][j]\n",
    "                \n",
    "        base_value = data.iloc[day_counter + historical_size - 1]['SP500 Adj Close']\n",
    "        future_values = []\n",
    "        for i in range (prediction_size):\n",
    "            future_values += [data.iloc[day_counter + historical_size + i]['SP500 Adj Close']] \n",
    "        average_future = sum(future_values) / len(future_values)\n",
    "        if ((average_future - base_value) > 0):\n",
    "            delta = 1\n",
    "        else:\n",
    "            delta = 0\n",
    "            \n",
    "        y_train[day_counter] = delta\n",
    "        \n",
    "        sanity_train[day_counter] = np.array(future_values)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        \n",
    "        for i in range (historical_size):\n",
    "            for j in range (daily_data_size):\n",
    "                x_test[day_counter - len(x_train)][i][j] = data.iloc[day_counter + i][j]\n",
    "                \n",
    "        base_value = data.iloc[day_counter + historical_size - 1]['SP500 Adj Close']\n",
    "        future_values = []\n",
    "        for i in range (prediction_size):\n",
    "            future_values += [data.iloc[day_counter + historical_size + i]['SP500 Adj Close']] \n",
    "        average_future = sum(future_values) / len(future_values)\n",
    "        if ((average_future - base_value) > 0):\n",
    "            delta = 1\n",
    "        else:\n",
    "            delta = 0\n",
    "            \n",
    "        y_test[day_counter - len(y_train)] = delta\n",
    "        \n",
    "        sanity_test[day_counter - len(y_train)] = np.array(future_values)\n",
    "        \n",
    "    day_counter += 1\n",
    "    print(\"{}% completed\".format(int(100 * day_counter/data_size)), end = '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train = y_train.astype('uint8')\n",
    "y_test  = y_test.astype('uint8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance out training data\n",
    "ones = 0\n",
    "zeros = 0\n",
    "for i in y_train:\n",
    "    if (i == 1):\n",
    "        ones += 1\n",
    "    else:\n",
    "        zeros += 1\n",
    "balanced_x_train = np.zeros((2*min(ones,zeros), historical_size, daily_data_size))\n",
    "balanced_y_train = np.zeros((2*min(ones,zeros)))\n",
    "balanced_sanity_train = np.zeros((2*min(ones,zeros), 3))\n",
    "\n",
    "# Insert ones\n",
    "index = 0\n",
    "for i in range (len(x_train)):\n",
    "    if (y_train[i] == 1):\n",
    "        balanced_x_train[index] = x_train[i]\n",
    "        balanced_y_train[index] = y_train[i]\n",
    "        balanced_sanity_train[index] = sanity_train[i]\n",
    "        index += 1\n",
    "        \n",
    "    if (index == min(ones, zeros)):\n",
    "        break\n",
    "        \n",
    "# Insert zeros\n",
    "for i in range (len(x_train)):\n",
    "    if (y_train[i] == 0):\n",
    "        balanced_x_train[index] = x_train[i]\n",
    "        balanced_y_train[index] = y_train[i]\n",
    "        balanced_sanity_train[index] = sanity_train[i]\n",
    "        index += 1\n",
    "        \n",
    "    if (index == len(balanced_x_train)):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance out testing data\n",
    "ones = 0\n",
    "zeros = 0\n",
    "for i in y_test:\n",
    "    if (i == 1):\n",
    "        ones += 1\n",
    "    else:\n",
    "        zeros += 1\n",
    "balanced_x_test = np.zeros((2*min(ones,zeros), historical_size, daily_data_size))\n",
    "balanced_y_test = np.zeros((2*min(ones,zeros)))\n",
    "balanced_sanity_test = np.zeros((2*min(ones,zeros), 3))\n",
    "\n",
    "# Insert ones\n",
    "index = 0\n",
    "for i in range (len(x_test)):\n",
    "    if (y_test[i] == 1):\n",
    "        balanced_x_test[index] = x_test[i]\n",
    "        balanced_y_test[index] = y_test[i]\n",
    "        balanced_sanity_test[index] = sanity_test[i]\n",
    "        index += 1\n",
    "        \n",
    "    if (index == min(ones, zeros)):\n",
    "        break\n",
    "        \n",
    "# Insert zeros\n",
    "for i in range (len(x_test)):\n",
    "    if (y_test[i] == 0):\n",
    "        balanced_x_test[index] = x_test[i]\n",
    "        balanced_y_test[index] = y_test[i]\n",
    "        balanced_sanity_test[index] = sanity_test[i]\n",
    "        index += 1\n",
    "        \n",
    "    if (index == len(balanced_x_test)):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322\n",
      "322\n"
     ]
    }
   ],
   "source": [
    "ones = 0\n",
    "zeros = 0\n",
    "for i in balanced_y_test:\n",
    "    if (i == 1):\n",
    "        ones += 1\n",
    "    else:\n",
    "        zeros += 1\n",
    "        \n",
    "print(ones)\n",
    "print(zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  \"\"\"\n",
      "c:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  \n",
      "c:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:11: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "c:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  if sys.path[0] == '':\n",
      "c:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "# Shuffle training data\n",
    "randomize_training = np.arange(len(balanced_x_train))\n",
    "np.random.shuffle(randomize_training)\n",
    "balanced_x_train = balanced_x_train[[randomize_training]]\n",
    "balanced_y_train = balanced_y_train[[randomize_training]]\n",
    "balanced_sanity_train = balanced_sanity_train[[randomize_training]]\n",
    "\n",
    "# Shuffle testing data\n",
    "randomize_testing = np.arange(len(balanced_x_test))\n",
    "np.random.shuffle(randomize_testing)\n",
    "balanced_x_test = balanced_x_test[[randomize_testing]]\n",
    "balanced_y_test = balanced_y_test[[randomize_testing]]\n",
    "balanced_sanity_test = balanced_sanity_test[[randomize_testing]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_y_train = balanced_y_train.astype('uint8')\n",
    "balanced_y_test  = balanced_y_test.astype('uint8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize training data\n",
    "\n",
    "def find_max_in_col(df, column):\n",
    "    data = []\n",
    "    for sequence in df:\n",
    "        for day in sequence:\n",
    "            data.append(day[column])\n",
    "    return max(data)\n",
    "\n",
    "# 1 - Find the maximums of each column\n",
    "maximums = []\n",
    "for daily_feature in range (daily_data_size):\n",
    "    maximums += [find_max_in_col(balanced_x_train, daily_feature)]\n",
    "    \n",
    "# 2 - Divide each number by the maximum\n",
    "for sequence in range (len(balanced_x_train)):\n",
    "    for day in range (historical_size):\n",
    "        for daily_feature in range (daily_data_size):\n",
    "            balanced_x_train[sequence][day][daily_feature] = balanced_x_train[sequence][day][daily_feature] / maximums[daily_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize testing data\n",
    "\n",
    "def find_max_in_col(df, column):\n",
    "    data = []\n",
    "    for sequence in df:\n",
    "        for day in sequence:\n",
    "            data.append(day[column])\n",
    "    return max(data)\n",
    "\n",
    "# 1 - Find the maximums of each column\n",
    "maximums = []\n",
    "for daily_feature in range (daily_data_size):\n",
    "    maximums += [find_max_in_col(balanced_x_test, daily_feature)]\n",
    "    \n",
    "# 2 - Divide each number by the maximum\n",
    "for sequence in range (len(balanced_x_test)):\n",
    "    for day in range (historical_size):\n",
    "        for daily_feature in range (daily_data_size):\n",
    "            balanced_x_test[sequence][day][daily_feature] = balanced_x_test[sequence][day][daily_feature] / maximums[daily_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.5)-optimizer(Adam)-learning_rate(5e-05)-batch_size(32)\n",
      "-------------------------------------------\n",
      "2 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.5)-optimizer(Adam)-learning_rate(5e-05)-batch_size(64)\n",
      "-------------------------------------------\n",
      "3 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.5)-optimizer(Adam)-learning_rate(5e-05)-batch_size(128)\n",
      "-------------------------------------------\n",
      "4 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.5)-optimizer(Adam)-learning_rate(0.0001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "5 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.5)-optimizer(Adam)-learning_rate(0.0001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "6 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.5)-optimizer(Adam)-learning_rate(0.0001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "7 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.5)-optimizer(Adam)-learning_rate(0.0005)-batch_size(32)\n",
      "-------------------------------------------\n",
      "8 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.5)-optimizer(Adam)-learning_rate(0.0005)-batch_size(64)\n",
      "-------------------------------------------\n",
      "9 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.5)-optimizer(Adam)-learning_rate(0.0005)-batch_size(128)\n",
      "-------------------------------------------\n",
      "10 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.5)-optimizer(Adam)-learning_rate(0.001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "11 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.5)-optimizer(Adam)-learning_rate(0.001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "12 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.5)-optimizer(Adam)-learning_rate(0.001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "13 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.5)-optimizer(SGD)-learning_rate(5e-05)-batch_size(32)\n",
      "-------------------------------------------\n",
      "14 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.5)-optimizer(SGD)-learning_rate(5e-05)-batch_size(64)\n",
      "-------------------------------------------\n",
      "15 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.5)-optimizer(SGD)-learning_rate(5e-05)-batch_size(128)\n",
      "-------------------------------------------\n",
      "16 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.5)-optimizer(SGD)-learning_rate(0.0001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "17 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.5)-optimizer(SGD)-learning_rate(0.0001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "18 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.5)-optimizer(SGD)-learning_rate(0.0001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "19 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.5)-optimizer(SGD)-learning_rate(0.0005)-batch_size(32)\n",
      "-------------------------------------------\n",
      "20 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.5)-optimizer(SGD)-learning_rate(0.0005)-batch_size(64)\n",
      "-------------------------------------------\n",
      "21 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.5)-optimizer(SGD)-learning_rate(0.0005)-batch_size(128)\n",
      "-------------------------------------------\n",
      "22 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.5)-optimizer(SGD)-learning_rate(0.001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "23 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.5)-optimizer(SGD)-learning_rate(0.001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "24 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.5)-optimizer(SGD)-learning_rate(0.001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "25 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.75)-optimizer(Adam)-learning_rate(5e-05)-batch_size(32)\n",
      "-------------------------------------------\n",
      "26 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.75)-optimizer(Adam)-learning_rate(5e-05)-batch_size(64)\n",
      "-------------------------------------------\n",
      "27 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.75)-optimizer(Adam)-learning_rate(5e-05)-batch_size(128)\n",
      "-------------------------------------------\n",
      "28 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.75)-optimizer(Adam)-learning_rate(0.0001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "29 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.75)-optimizer(Adam)-learning_rate(0.0001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "30 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.75)-optimizer(Adam)-learning_rate(0.0001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "31 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.75)-optimizer(Adam)-learning_rate(0.0005)-batch_size(32)\n",
      "-------------------------------------------\n",
      "32 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.75)-optimizer(Adam)-learning_rate(0.0005)-batch_size(64)\n",
      "-------------------------------------------\n",
      "33 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.75)-optimizer(Adam)-learning_rate(0.0005)-batch_size(128)\n",
      "-------------------------------------------\n",
      "34 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.75)-optimizer(Adam)-learning_rate(0.001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "35 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.75)-optimizer(Adam)-learning_rate(0.001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "36 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.75)-optimizer(Adam)-learning_rate(0.001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "37 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.75)-optimizer(SGD)-learning_rate(5e-05)-batch_size(32)\n",
      "-------------------------------------------\n",
      "38 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.75)-optimizer(SGD)-learning_rate(5e-05)-batch_size(64)\n",
      "-------------------------------------------\n",
      "39 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.75)-optimizer(SGD)-learning_rate(5e-05)-batch_size(128)\n",
      "-------------------------------------------\n",
      "40 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.75)-optimizer(SGD)-learning_rate(0.0001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "41 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.75)-optimizer(SGD)-learning_rate(0.0001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "42 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.75)-optimizer(SGD)-learning_rate(0.0001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "43 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.75)-optimizer(SGD)-learning_rate(0.0005)-batch_size(32)\n",
      "-------------------------------------------\n",
      "44 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.75)-optimizer(SGD)-learning_rate(0.0005)-batch_size(64)\n",
      "-------------------------------------------\n",
      "45 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.75)-optimizer(SGD)-learning_rate(0.0005)-batch_size(128)\n",
      "-------------------------------------------\n",
      "46 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.75)-optimizer(SGD)-learning_rate(0.001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "47 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.75)-optimizer(SGD)-learning_rate(0.001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "48 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(0.75)-optimizer(SGD)-learning_rate(0.001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "49 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1)-optimizer(Adam)-learning_rate(5e-05)-batch_size(32)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1)-optimizer(Adam)-learning_rate(5e-05)-batch_size(64)\n",
      "-------------------------------------------\n",
      "51 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1)-optimizer(Adam)-learning_rate(5e-05)-batch_size(128)\n",
      "-------------------------------------------\n",
      "52 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1)-optimizer(Adam)-learning_rate(0.0001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "53 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1)-optimizer(Adam)-learning_rate(0.0001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "54 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1)-optimizer(Adam)-learning_rate(0.0001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "55 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1)-optimizer(Adam)-learning_rate(0.0005)-batch_size(32)\n",
      "-------------------------------------------\n",
      "56 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1)-optimizer(Adam)-learning_rate(0.0005)-batch_size(64)\n",
      "-------------------------------------------\n",
      "57 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1)-optimizer(Adam)-learning_rate(0.0005)-batch_size(128)\n",
      "-------------------------------------------\n",
      "58 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1)-optimizer(Adam)-learning_rate(0.001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "59 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1)-optimizer(Adam)-learning_rate(0.001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "60 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1)-optimizer(Adam)-learning_rate(0.001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "61 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1)-optimizer(SGD)-learning_rate(5e-05)-batch_size(32)\n",
      "-------------------------------------------\n",
      "62 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1)-optimizer(SGD)-learning_rate(5e-05)-batch_size(64)\n",
      "-------------------------------------------\n",
      "63 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1)-optimizer(SGD)-learning_rate(5e-05)-batch_size(128)\n",
      "-------------------------------------------\n",
      "64 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1)-optimizer(SGD)-learning_rate(0.0001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "65 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1)-optimizer(SGD)-learning_rate(0.0001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "66 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1)-optimizer(SGD)-learning_rate(0.0001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "67 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1)-optimizer(SGD)-learning_rate(0.0005)-batch_size(32)\n",
      "-------------------------------------------\n",
      "68 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1)-optimizer(SGD)-learning_rate(0.0005)-batch_size(64)\n",
      "-------------------------------------------\n",
      "69 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1)-optimizer(SGD)-learning_rate(0.0005)-batch_size(128)\n",
      "-------------------------------------------\n",
      "70 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1)-optimizer(SGD)-learning_rate(0.001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "71 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1)-optimizer(SGD)-learning_rate(0.001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "72 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1)-optimizer(SGD)-learning_rate(0.001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "73 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1.25)-optimizer(Adam)-learning_rate(5e-05)-batch_size(32)\n",
      "-------------------------------------------\n",
      "74 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1.25)-optimizer(Adam)-learning_rate(5e-05)-batch_size(64)\n",
      "-------------------------------------------\n",
      "75 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1.25)-optimizer(Adam)-learning_rate(5e-05)-batch_size(128)\n",
      "-------------------------------------------\n",
      "76 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1.25)-optimizer(Adam)-learning_rate(0.0001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "77 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1.25)-optimizer(Adam)-learning_rate(0.0001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "78 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1.25)-optimizer(Adam)-learning_rate(0.0001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "79 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1.25)-optimizer(Adam)-learning_rate(0.0005)-batch_size(32)\n",
      "-------------------------------------------\n",
      "80 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1.25)-optimizer(Adam)-learning_rate(0.0005)-batch_size(64)\n",
      "-------------------------------------------\n",
      "81 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1.25)-optimizer(Adam)-learning_rate(0.0005)-batch_size(128)\n",
      "-------------------------------------------\n",
      "82 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1.25)-optimizer(Adam)-learning_rate(0.001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "83 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1.25)-optimizer(Adam)-learning_rate(0.001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "84 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1.25)-optimizer(Adam)-learning_rate(0.001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "85 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1.25)-optimizer(SGD)-learning_rate(5e-05)-batch_size(32)\n",
      "-------------------------------------------\n",
      "86 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1.25)-optimizer(SGD)-learning_rate(5e-05)-batch_size(64)\n",
      "-------------------------------------------\n",
      "87 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1.25)-optimizer(SGD)-learning_rate(5e-05)-batch_size(128)\n",
      "-------------------------------------------\n",
      "88 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1.25)-optimizer(SGD)-learning_rate(0.0001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "89 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1.25)-optimizer(SGD)-learning_rate(0.0001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "90 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1.25)-optimizer(SGD)-learning_rate(0.0001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "91 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1.25)-optimizer(SGD)-learning_rate(0.0005)-batch_size(32)\n",
      "-------------------------------------------\n",
      "92 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1.25)-optimizer(SGD)-learning_rate(0.0005)-batch_size(64)\n",
      "-------------------------------------------\n",
      "93 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1.25)-optimizer(SGD)-learning_rate(0.0005)-batch_size(128)\n",
      "-------------------------------------------\n",
      "94 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1.25)-optimizer(SGD)-learning_rate(0.001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "95 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1.25)-optimizer(SGD)-learning_rate(0.001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "96 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(1.25)-optimizer(SGD)-learning_rate(0.001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "97 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(2)-optimizer(Adam)-learning_rate(5e-05)-batch_size(32)\n",
      "-------------------------------------------\n",
      "98 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(2)-optimizer(Adam)-learning_rate(5e-05)-batch_size(64)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(2)-optimizer(Adam)-learning_rate(5e-05)-batch_size(128)\n",
      "-------------------------------------------\n",
      "100 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(2)-optimizer(Adam)-learning_rate(0.0001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "101 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(2)-optimizer(Adam)-learning_rate(0.0001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "102 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(2)-optimizer(Adam)-learning_rate(0.0001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "103 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(2)-optimizer(Adam)-learning_rate(0.0005)-batch_size(32)\n",
      "-------------------------------------------\n",
      "104 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(2)-optimizer(Adam)-learning_rate(0.0005)-batch_size(64)\n",
      "-------------------------------------------\n",
      "105 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(2)-optimizer(Adam)-learning_rate(0.0005)-batch_size(128)\n",
      "-------------------------------------------\n",
      "106 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(2)-optimizer(Adam)-learning_rate(0.001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "107 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(2)-optimizer(Adam)-learning_rate(0.001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "108 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(2)-optimizer(Adam)-learning_rate(0.001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "109 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(2)-optimizer(SGD)-learning_rate(5e-05)-batch_size(32)\n",
      "-------------------------------------------\n",
      "110 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(2)-optimizer(SGD)-learning_rate(5e-05)-batch_size(64)\n",
      "-------------------------------------------\n",
      "111 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(2)-optimizer(SGD)-learning_rate(5e-05)-batch_size(128)\n",
      "-------------------------------------------\n",
      "112 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(2)-optimizer(SGD)-learning_rate(0.0001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "113 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(2)-optimizer(SGD)-learning_rate(0.0001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "114 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(2)-optimizer(SGD)-learning_rate(0.0001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "115 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(2)-optimizer(SGD)-learning_rate(0.0005)-batch_size(32)\n",
      "-------------------------------------------\n",
      "116 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(2)-optimizer(SGD)-learning_rate(0.0005)-batch_size(64)\n",
      "-------------------------------------------\n",
      "117 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(2)-optimizer(SGD)-learning_rate(0.0005)-batch_size(128)\n",
      "-------------------------------------------\n",
      "118 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(2)-optimizer(SGD)-learning_rate(0.001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "119 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(2)-optimizer(SGD)-learning_rate(0.001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "120 / 480 models\n",
      "Model: representation_space(32)-drop_out_multiplier(2)-optimizer(SGD)-learning_rate(0.001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "121 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.5)-optimizer(Adam)-learning_rate(5e-05)-batch_size(32)\n",
      "-------------------------------------------\n",
      "122 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.5)-optimizer(Adam)-learning_rate(5e-05)-batch_size(64)\n",
      "-------------------------------------------\n",
      "123 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.5)-optimizer(Adam)-learning_rate(5e-05)-batch_size(128)\n",
      "-------------------------------------------\n",
      "124 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.5)-optimizer(Adam)-learning_rate(0.0001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "125 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.5)-optimizer(Adam)-learning_rate(0.0001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "126 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.5)-optimizer(Adam)-learning_rate(0.0001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "127 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.5)-optimizer(Adam)-learning_rate(0.0005)-batch_size(32)\n",
      "-------------------------------------------\n",
      "128 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.5)-optimizer(Adam)-learning_rate(0.0005)-batch_size(64)\n",
      "-------------------------------------------\n",
      "129 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.5)-optimizer(Adam)-learning_rate(0.0005)-batch_size(128)\n",
      "-------------------------------------------\n",
      "130 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.5)-optimizer(Adam)-learning_rate(0.001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "131 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.5)-optimizer(Adam)-learning_rate(0.001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "132 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.5)-optimizer(Adam)-learning_rate(0.001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "133 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.5)-optimizer(SGD)-learning_rate(5e-05)-batch_size(32)\n",
      "-------------------------------------------\n",
      "134 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.5)-optimizer(SGD)-learning_rate(5e-05)-batch_size(64)\n",
      "-------------------------------------------\n",
      "135 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.5)-optimizer(SGD)-learning_rate(5e-05)-batch_size(128)\n",
      "-------------------------------------------\n",
      "136 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.5)-optimizer(SGD)-learning_rate(0.0001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "137 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.5)-optimizer(SGD)-learning_rate(0.0001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "138 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.5)-optimizer(SGD)-learning_rate(0.0001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "139 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.5)-optimizer(SGD)-learning_rate(0.0005)-batch_size(32)\n",
      "-------------------------------------------\n",
      "140 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.5)-optimizer(SGD)-learning_rate(0.0005)-batch_size(64)\n",
      "-------------------------------------------\n",
      "141 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.5)-optimizer(SGD)-learning_rate(0.0005)-batch_size(128)\n",
      "-------------------------------------------\n",
      "142 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.5)-optimizer(SGD)-learning_rate(0.001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "143 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.5)-optimizer(SGD)-learning_rate(0.001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "144 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.5)-optimizer(SGD)-learning_rate(0.001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "145 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.75)-optimizer(Adam)-learning_rate(5e-05)-batch_size(32)\n",
      "-------------------------------------------\n",
      "146 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.75)-optimizer(Adam)-learning_rate(5e-05)-batch_size(64)\n",
      "-------------------------------------------\n",
      "147 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.75)-optimizer(Adam)-learning_rate(5e-05)-batch_size(128)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.75)-optimizer(Adam)-learning_rate(0.0001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "149 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.75)-optimizer(Adam)-learning_rate(0.0001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "150 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.75)-optimizer(Adam)-learning_rate(0.0001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "151 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.75)-optimizer(Adam)-learning_rate(0.0005)-batch_size(32)\n",
      "-------------------------------------------\n",
      "152 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.75)-optimizer(Adam)-learning_rate(0.0005)-batch_size(64)\n",
      "-------------------------------------------\n",
      "153 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.75)-optimizer(Adam)-learning_rate(0.0005)-batch_size(128)\n",
      "-------------------------------------------\n",
      "154 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.75)-optimizer(Adam)-learning_rate(0.001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "155 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.75)-optimizer(Adam)-learning_rate(0.001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "156 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.75)-optimizer(Adam)-learning_rate(0.001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "157 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.75)-optimizer(SGD)-learning_rate(5e-05)-batch_size(32)\n",
      "-------------------------------------------\n",
      "158 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.75)-optimizer(SGD)-learning_rate(5e-05)-batch_size(64)\n",
      "-------------------------------------------\n",
      "159 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.75)-optimizer(SGD)-learning_rate(5e-05)-batch_size(128)\n",
      "-------------------------------------------\n",
      "160 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.75)-optimizer(SGD)-learning_rate(0.0001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "161 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.75)-optimizer(SGD)-learning_rate(0.0001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "162 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.75)-optimizer(SGD)-learning_rate(0.0001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "163 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.75)-optimizer(SGD)-learning_rate(0.0005)-batch_size(32)\n",
      "-------------------------------------------\n",
      "164 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.75)-optimizer(SGD)-learning_rate(0.0005)-batch_size(64)\n",
      "-------------------------------------------\n",
      "165 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.75)-optimizer(SGD)-learning_rate(0.0005)-batch_size(128)\n",
      "-------------------------------------------\n",
      "166 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.75)-optimizer(SGD)-learning_rate(0.001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "167 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.75)-optimizer(SGD)-learning_rate(0.001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "168 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(0.75)-optimizer(SGD)-learning_rate(0.001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "169 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(1)-optimizer(Adam)-learning_rate(5e-05)-batch_size(32)\n",
      "-------------------------------------------\n",
      "170 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(1)-optimizer(Adam)-learning_rate(5e-05)-batch_size(64)\n",
      "-------------------------------------------\n",
      "171 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(1)-optimizer(Adam)-learning_rate(5e-05)-batch_size(128)\n",
      "-------------------------------------------\n",
      "172 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(1)-optimizer(Adam)-learning_rate(0.0001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "173 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(1)-optimizer(Adam)-learning_rate(0.0001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "174 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(1)-optimizer(Adam)-learning_rate(0.0001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "175 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(1)-optimizer(Adam)-learning_rate(0.0005)-batch_size(32)\n",
      "-------------------------------------------\n",
      "176 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(1)-optimizer(Adam)-learning_rate(0.0005)-batch_size(64)\n",
      "-------------------------------------------\n",
      "177 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(1)-optimizer(Adam)-learning_rate(0.0005)-batch_size(128)\n",
      "-------------------------------------------\n",
      "178 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(1)-optimizer(Adam)-learning_rate(0.001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "179 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(1)-optimizer(Adam)-learning_rate(0.001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "180 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(1)-optimizer(Adam)-learning_rate(0.001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "181 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(1)-optimizer(SGD)-learning_rate(5e-05)-batch_size(32)\n",
      "-------------------------------------------\n",
      "182 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(1)-optimizer(SGD)-learning_rate(5e-05)-batch_size(64)\n",
      "-------------------------------------------\n",
      "183 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(1)-optimizer(SGD)-learning_rate(5e-05)-batch_size(128)\n",
      "-------------------------------------------\n",
      "184 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(1)-optimizer(SGD)-learning_rate(0.0001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "185 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(1)-optimizer(SGD)-learning_rate(0.0001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "186 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(1)-optimizer(SGD)-learning_rate(0.0001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "187 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(1)-optimizer(SGD)-learning_rate(0.0005)-batch_size(32)\n",
      "-------------------------------------------\n",
      "188 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(1)-optimizer(SGD)-learning_rate(0.0005)-batch_size(64)\n",
      "-------------------------------------------\n",
      "189 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(1)-optimizer(SGD)-learning_rate(0.0005)-batch_size(128)\n",
      "-------------------------------------------\n",
      "190 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(1)-optimizer(SGD)-learning_rate(0.001)-batch_size(32)\n",
      "-------------------------------------------\n",
      "191 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(1)-optimizer(SGD)-learning_rate(0.001)-batch_size(64)\n",
      "-------------------------------------------\n",
      "192 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(1)-optimizer(SGD)-learning_rate(0.001)-batch_size(128)\n",
      "-------------------------------------------\n",
      "193 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(1.25)-optimizer(Adam)-learning_rate(5e-05)-batch_size(32)\n",
      "-------------------------------------------\n",
      "194 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(1.25)-optimizer(Adam)-learning_rate(5e-05)-batch_size(64)\n",
      "-------------------------------------------\n",
      "195 / 480 models\n",
      "Model: representation_space(64)-drop_out_multiplier(1.25)-optimizer(Adam)-learning_rate(5e-05)-batch_size(128)\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "representation_spaces = [32,64,128,256]\n",
    "drop_out_multipliers = [1/2, 3/4, 1, 5/4, 2]\n",
    "optimizers = ['Adam', 'SGD']\n",
    "learning_rates = [0.00005, 0.0001, 0.0005,0.001]\n",
    "batch_sizes = [32, 64, 128] \n",
    "\n",
    "# Maps model names to their validation accuracy\n",
    "results = {}\n",
    "\n",
    "counter = 1\n",
    "for representation_space in representation_spaces:\n",
    "    for drop_out_multiplier in drop_out_multipliers:\n",
    "        for optimizer in optimizers:\n",
    "            for learning_rate in learning_rates:\n",
    "                for batch_size in batch_sizes:\n",
    "                \n",
    "                    NAME = \"representation_space({})-drop_out_multiplier({})-optimizer({})-learning_rate({})-batch_size({})\".format( \\\n",
    "                        representation_space,drop_out_multiplier, optimizer, learning_rate, batch_size)\n",
    "                    \n",
    "                    print(\"{} / {} models\".format(counter, \n",
    "len(drop_out_multipliers) * len(optimizers) * len(learning_rates) * len(batch_sizes) * len(representation_spaces)))\n",
    "                    print(\"Model: {}\".format(NAME))\n",
    "                    \n",
    "                    print(\"-------------------------------------------\")\n",
    "                    \n",
    "                    model = Sequential()\n",
    "\n",
    "                    model.add(LSTM(representation_space, input_shape = balanced_x_train.shape[1:], return_sequences = True))\n",
    "                    model.add(Dropout(0.2 * drop_out_multiplier))\n",
    "                    model.add(BatchNormalization())\n",
    "\n",
    "                    model.add(LSTM(representation_space, return_sequences = True))\n",
    "                    model.add(Dropout(0.1 * drop_out_multiplier))\n",
    "                    model.add(BatchNormalization())\n",
    "\n",
    "                    model.add(LSTM(representation_space))\n",
    "                    model.add(Dropout(0.2 * drop_out_multiplier))\n",
    "                    model.add(BatchNormalization())\n",
    "\n",
    "                    model.add(Dense(32, activation = 'relu'))\n",
    "                    model.add(Dropout(0.2 * drop_out_multiplier))\n",
    "\n",
    "                    model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "                    if optimizer == 'Adam':\n",
    "                        opt = tf.keras.optimizers.Adam(lr=learning_rate, decay=1e-6)\n",
    "\n",
    "                    else:\n",
    "                        opt  = tf.keras.optimizers.SGD(learning_rate = 0.001, momentum = 0)\n",
    "\n",
    "\n",
    "\n",
    "                    model.compile(\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        optimizer=opt,\n",
    "                        metrics=['accuracy']\n",
    "                    )\n",
    "\n",
    "                    \n",
    "\n",
    "                    tensorboard = TensorBoard(log_dir=\"logs\\{}\".format(NAME), profile_batch=0)\n",
    "\n",
    "                    history = model.fit(\n",
    "                        balanced_x_train, balanced_y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=300,\n",
    "                        validation_data=(balanced_x_test, balanced_y_test),\n",
    "                        callbacks = [tensorboard],\n",
    "                        verbose = 0\n",
    "                    )\n",
    "                    \n",
    "                    score = model.evaluate(balanced_x_test, balanced_y_test, verbose=0)\n",
    "                    \n",
    "                    results[NAME] = score[1]\n",
    "                    \n",
    "                    #model.save(\"models\\{}\".format(NAME))\n",
    "                    \n",
    "                    counter += 1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " 2/60 [>.............................] - ETA: 10s - loss: 0.9769 - accuracy: 0.4688WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.149872). Check your callbacks.\n",
      "60/60 [==============================] - 8s 127ms/step - loss: 0.8701 - accuracy: 0.5147 - val_loss: 0.6949 - val_accuracy: 0.5000\n",
      "Epoch 2/500\n",
      "60/60 [==============================] - 6s 103ms/step - loss: 0.8101 - accuracy: 0.5273 - val_loss: 0.7022 - val_accuracy: 0.5000\n",
      "Epoch 3/500\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.7806 - accuracy: 0.5575 - val_loss: 0.7095 - val_accuracy: 0.5000\n",
      "Epoch 4/500\n",
      "60/60 [==============================] - 3s 55ms/step - loss: 0.7890 - accuracy: 0.5436 - val_loss: 0.7148 - val_accuracy: 0.5000\n",
      "Epoch 5/500\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.7846 - accuracy: 0.5278 - val_loss: 0.7007 - val_accuracy: 0.5100\n",
      "Epoch 6/500\n",
      "60/60 [==============================] - 3s 58ms/step - loss: 0.7573 - accuracy: 0.5433 - val_loss: 0.6826 - val_accuracy: 0.5515\n",
      "Epoch 7/500\n",
      "60/60 [==============================] - 3s 55ms/step - loss: 0.7546 - accuracy: 0.5441 - val_loss: 0.6868 - val_accuracy: 0.5150\n",
      "Epoch 8/500\n",
      "60/60 [==============================] - 3s 55ms/step - loss: 0.7518 - accuracy: 0.5378 - val_loss: 0.7004 - val_accuracy: 0.5017\n",
      "Epoch 9/500\n",
      "60/60 [==============================] - 3s 55ms/step - loss: 0.7502 - accuracy: 0.5415 - val_loss: 0.7034 - val_accuracy: 0.4867\n",
      "Epoch 10/500\n",
      "60/60 [==============================] - 3s 55ms/step - loss: 0.7369 - accuracy: 0.5499 - val_loss: 0.6879 - val_accuracy: 0.5133\n",
      "Epoch 11/500\n",
      "60/60 [==============================] - 3s 55ms/step - loss: 0.7497 - accuracy: 0.5472 - val_loss: 0.6973 - val_accuracy: 0.4983\n",
      "Epoch 12/500\n",
      "60/60 [==============================] - 3s 54ms/step - loss: 0.7250 - accuracy: 0.5551 - val_loss: 0.6858 - val_accuracy: 0.5565\n",
      "Epoch 13/500\n",
      "60/60 [==============================] - 3s 55ms/step - loss: 0.7239 - accuracy: 0.5588 - val_loss: 0.6928 - val_accuracy: 0.5299\n",
      "Epoch 14/500\n",
      "60/60 [==============================] - 3s 55ms/step - loss: 0.7234 - accuracy: 0.5554 - val_loss: 0.6987 - val_accuracy: 0.5831\n",
      "Epoch 15/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.7369 - accuracy: 0.5365 - val_loss: 0.7100 - val_accuracy: 0.5482\n",
      "Epoch 16/500\n",
      "60/60 [==============================] - 3s 55ms/step - loss: 0.7284 - accuracy: 0.5520 - val_loss: 0.7682 - val_accuracy: 0.4950\n",
      "Epoch 17/500\n",
      "60/60 [==============================] - 3s 55ms/step - loss: 0.7116 - accuracy: 0.5585 - val_loss: 0.7960 - val_accuracy: 0.5116\n",
      "Epoch 18/500\n",
      "60/60 [==============================] - 3s 55ms/step - loss: 0.7131 - accuracy: 0.5507 - val_loss: 0.7342 - val_accuracy: 0.5332\n",
      "Epoch 19/500\n",
      "60/60 [==============================] - 3s 55ms/step - loss: 0.7175 - accuracy: 0.5441 - val_loss: 0.7172 - val_accuracy: 0.5233\n",
      "Epoch 20/500\n",
      "60/60 [==============================] - 3s 54ms/step - loss: 0.6976 - accuracy: 0.5606 - val_loss: 0.7069 - val_accuracy: 0.5465\n",
      "Epoch 21/500\n",
      "60/60 [==============================] - 3s 55ms/step - loss: 0.7076 - accuracy: 0.5475 - val_loss: 0.7061 - val_accuracy: 0.5831\n",
      "Epoch 22/500\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.7097 - accuracy: 0.5551 - val_loss: 0.7501 - val_accuracy: 0.5166\n",
      "Epoch 23/500\n",
      "60/60 [==============================] - 4s 60ms/step - loss: 0.7067 - accuracy: 0.5546 - val_loss: 0.7435 - val_accuracy: 0.5183\n",
      "Epoch 24/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.7029 - accuracy: 0.5551 - val_loss: 0.7452 - val_accuracy: 0.5266\n",
      "Epoch 25/500\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.7003 - accuracy: 0.5459 - val_loss: 0.7637 - val_accuracy: 0.5100\n",
      "Epoch 26/500\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.6967 - accuracy: 0.5530 - val_loss: 0.7140 - val_accuracy: 0.5714\n",
      "Epoch 27/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.7003 - accuracy: 0.5407 - val_loss: 0.7024 - val_accuracy: 0.5748\n",
      "Epoch 28/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6944 - accuracy: 0.5549 - val_loss: 0.6802 - val_accuracy: 0.5764\n",
      "Epoch 29/500\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.6849 - accuracy: 0.5549 - val_loss: 0.6800 - val_accuracy: 0.5997\n",
      "Epoch 30/500\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.6826 - accuracy: 0.5583 - val_loss: 0.6918 - val_accuracy: 0.5831\n",
      "Epoch 31/500\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.6904 - accuracy: 0.5554 - val_loss: 0.7001 - val_accuracy: 0.5681\n",
      "Epoch 32/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6909 - accuracy: 0.5614 - val_loss: 0.6736 - val_accuracy: 0.6013\n",
      "Epoch 33/500\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.6811 - accuracy: 0.5570 - val_loss: 0.6840 - val_accuracy: 0.5681\n",
      "Epoch 34/500\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.6912 - accuracy: 0.5415 - val_loss: 0.6971 - val_accuracy: 0.5797\n",
      "Epoch 35/500\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.6825 - accuracy: 0.5648 - val_loss: 0.6985 - val_accuracy: 0.5797\n",
      "Epoch 36/500\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.6921 - accuracy: 0.5396 - val_loss: 0.6964 - val_accuracy: 0.5864\n",
      "Epoch 37/500\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.6891 - accuracy: 0.5504 - val_loss: 0.6954 - val_accuracy: 0.5764\n",
      "Epoch 38/500\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.6804 - accuracy: 0.5564 - val_loss: 0.7150 - val_accuracy: 0.5465\n",
      "Epoch 39/500\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.6797 - accuracy: 0.5504 - val_loss: 0.6900 - val_accuracy: 0.5681\n",
      "Epoch 40/500\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.6812 - accuracy: 0.5672 - val_loss: 0.6996 - val_accuracy: 0.5681\n",
      "Epoch 41/500\n",
      "60/60 [==============================] - 3s 58ms/step - loss: 0.6813 - accuracy: 0.5541 - val_loss: 0.7102 - val_accuracy: 0.5548\n",
      "Epoch 42/500\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.6745 - accuracy: 0.5622 - val_loss: 0.6836 - val_accuracy: 0.5797\n",
      "Epoch 43/500\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.6807 - accuracy: 0.5591 - val_loss: 0.6765 - val_accuracy: 0.5598\n",
      "Epoch 44/500\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.6773 - accuracy: 0.5543 - val_loss: 0.6886 - val_accuracy: 0.5698\n",
      "Epoch 45/500\n",
      "60/60 [==============================] - 5s 88ms/step - loss: 0.6801 - accuracy: 0.5549 - val_loss: 0.6944 - val_accuracy: 0.5648\n",
      "Epoch 46/500\n",
      "60/60 [==============================] - 4s 71ms/step - loss: 0.6754 - accuracy: 0.5614 - val_loss: 0.6818 - val_accuracy: 0.5648\n",
      "Epoch 47/500\n",
      "60/60 [==============================] - 4s 70ms/step - loss: 0.6762 - accuracy: 0.5585 - val_loss: 0.6774 - val_accuracy: 0.5714\n",
      "Epoch 48/500\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.6719 - accuracy: 0.5604 - val_loss: 0.6726 - val_accuracy: 0.5681\n",
      "Epoch 49/500\n",
      "60/60 [==============================] - 4s 70ms/step - loss: 0.6731 - accuracy: 0.5661 - val_loss: 0.6812 - val_accuracy: 0.5266\n",
      "Epoch 50/500\n",
      "60/60 [==============================] - 5s 80ms/step - loss: 0.6710 - accuracy: 0.5454 - val_loss: 0.6919 - val_accuracy: 0.4950\n",
      "Epoch 51/500\n",
      "60/60 [==============================] - 5s 87ms/step - loss: 0.6661 - accuracy: 0.5551 - val_loss: 0.6590 - val_accuracy: 0.5963\n",
      "Epoch 52/500\n",
      "60/60 [==============================] - 4s 73ms/step - loss: 0.6691 - accuracy: 0.5567 - val_loss: 0.6722 - val_accuracy: 0.5781\n",
      "Epoch 53/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6632 - accuracy: 0.5690 - val_loss: 0.6666 - val_accuracy: 0.5997\n",
      "Epoch 54/500\n",
      "60/60 [==============================] - 4s 71ms/step - loss: 0.6709 - accuracy: 0.5517 - val_loss: 0.6538 - val_accuracy: 0.6013\n",
      "Epoch 55/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6689 - accuracy: 0.5609 - val_loss: 0.6588 - val_accuracy: 0.6013\n",
      "Epoch 56/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6621 - accuracy: 0.5706 - val_loss: 0.6582 - val_accuracy: 0.5963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "60/60 [==============================] - 4s 62ms/step - loss: 0.6760 - accuracy: 0.5549 - val_loss: 0.6644 - val_accuracy: 0.5930\n",
      "Epoch 58/500\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.6667 - accuracy: 0.5672 - val_loss: 0.6638 - val_accuracy: 0.6047\n",
      "Epoch 59/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6719 - accuracy: 0.5583 - val_loss: 0.6633 - val_accuracy: 0.6047\n",
      "Epoch 60/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6654 - accuracy: 0.5643 - val_loss: 0.6586 - val_accuracy: 0.5947\n",
      "Epoch 61/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6656 - accuracy: 0.5546 - val_loss: 0.6563 - val_accuracy: 0.6047\n",
      "Epoch 62/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6651 - accuracy: 0.5656 - val_loss: 0.6818 - val_accuracy: 0.5764\n",
      "Epoch 63/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6645 - accuracy: 0.5709 - val_loss: 0.6624 - val_accuracy: 0.6013\n",
      "Epoch 64/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6685 - accuracy: 0.5520 - val_loss: 0.6679 - val_accuracy: 0.5880\n",
      "Epoch 65/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6674 - accuracy: 0.5535 - val_loss: 0.6717 - val_accuracy: 0.6030\n",
      "Epoch 66/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6706 - accuracy: 0.5622 - val_loss: 0.6547 - val_accuracy: 0.6063\n",
      "Epoch 67/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6672 - accuracy: 0.5512 - val_loss: 0.6775 - val_accuracy: 0.5997\n",
      "Epoch 68/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6684 - accuracy: 0.5517 - val_loss: 0.6662 - val_accuracy: 0.5997\n",
      "Epoch 69/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6644 - accuracy: 0.5635 - val_loss: 0.6510 - val_accuracy: 0.6130\n",
      "Epoch 70/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6623 - accuracy: 0.5638 - val_loss: 0.6502 - val_accuracy: 0.6113\n",
      "Epoch 71/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6621 - accuracy: 0.5672 - val_loss: 0.6684 - val_accuracy: 0.5963\n",
      "Epoch 72/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6643 - accuracy: 0.5635 - val_loss: 0.6968 - val_accuracy: 0.5764\n",
      "Epoch 73/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6580 - accuracy: 0.5685 - val_loss: 0.6824 - val_accuracy: 0.5947\n",
      "Epoch 74/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6630 - accuracy: 0.5522 - val_loss: 0.6634 - val_accuracy: 0.6113\n",
      "Epoch 75/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6568 - accuracy: 0.5827 - val_loss: 0.6706 - val_accuracy: 0.5947\n",
      "Epoch 76/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6601 - accuracy: 0.5617 - val_loss: 0.6649 - val_accuracy: 0.5847\n",
      "Epoch 77/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6664 - accuracy: 0.5496 - val_loss: 0.6653 - val_accuracy: 0.5831\n",
      "Epoch 78/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6568 - accuracy: 0.5533 - val_loss: 0.6778 - val_accuracy: 0.5748\n",
      "Epoch 79/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6611 - accuracy: 0.5609 - val_loss: 0.6899 - val_accuracy: 0.5631\n",
      "Epoch 80/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6606 - accuracy: 0.5580 - val_loss: 0.6857 - val_accuracy: 0.5681\n",
      "Epoch 81/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6571 - accuracy: 0.5577 - val_loss: 0.6802 - val_accuracy: 0.5781\n",
      "Epoch 82/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6632 - accuracy: 0.5651 - val_loss: 0.6708 - val_accuracy: 0.5831\n",
      "Epoch 83/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6595 - accuracy: 0.5612 - val_loss: 0.6704 - val_accuracy: 0.5698\n",
      "Epoch 84/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6649 - accuracy: 0.5509 - val_loss: 0.6682 - val_accuracy: 0.5897\n",
      "Epoch 85/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6549 - accuracy: 0.5756 - val_loss: 0.6579 - val_accuracy: 0.6047\n",
      "Epoch 86/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6507 - accuracy: 0.5717 - val_loss: 0.6534 - val_accuracy: 0.6030\n",
      "Epoch 87/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6606 - accuracy: 0.5630 - val_loss: 0.6657 - val_accuracy: 0.5947\n",
      "Epoch 88/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6570 - accuracy: 0.5677 - val_loss: 0.6533 - val_accuracy: 0.6013\n",
      "Epoch 89/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6579 - accuracy: 0.5551 - val_loss: 0.6515 - val_accuracy: 0.5947\n",
      "Epoch 90/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6629 - accuracy: 0.5533 - val_loss: 0.6515 - val_accuracy: 0.5880\n",
      "Epoch 91/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6537 - accuracy: 0.5740 - val_loss: 0.6635 - val_accuracy: 0.5930\n",
      "Epoch 92/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6582 - accuracy: 0.5646 - val_loss: 0.6601 - val_accuracy: 0.5897\n",
      "Epoch 93/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6547 - accuracy: 0.5646 - val_loss: 0.6528 - val_accuracy: 0.6113\n",
      "Epoch 94/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6550 - accuracy: 0.5730 - val_loss: 0.6577 - val_accuracy: 0.5897\n",
      "Epoch 95/500\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.6590 - accuracy: 0.5669 - val_loss: 0.6555 - val_accuracy: 0.5847\n",
      "Epoch 96/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6579 - accuracy: 0.5633 - val_loss: 0.6511 - val_accuracy: 0.5963\n",
      "Epoch 97/500\n",
      "60/60 [==============================] - 5s 89ms/step - loss: 0.6498 - accuracy: 0.5732 - val_loss: 0.6538 - val_accuracy: 0.6030\n",
      "Epoch 98/500\n",
      "60/60 [==============================] - 5s 84ms/step - loss: 0.6594 - accuracy: 0.5680 - val_loss: 0.6592 - val_accuracy: 0.5997\n",
      "Epoch 99/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6529 - accuracy: 0.5703 - val_loss: 0.6558 - val_accuracy: 0.5897\n",
      "Epoch 100/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6597 - accuracy: 0.5588 - val_loss: 0.6641 - val_accuracy: 0.5997\n",
      "Epoch 101/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6550 - accuracy: 0.5612 - val_loss: 0.6601 - val_accuracy: 0.6047\n",
      "Epoch 102/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6597 - accuracy: 0.5504 - val_loss: 0.6606 - val_accuracy: 0.5831\n",
      "Epoch 103/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6539 - accuracy: 0.5738 - val_loss: 0.6557 - val_accuracy: 0.5781\n",
      "Epoch 104/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6590 - accuracy: 0.5567 - val_loss: 0.6531 - val_accuracy: 0.5847\n",
      "Epoch 105/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6568 - accuracy: 0.5627 - val_loss: 0.6527 - val_accuracy: 0.5797\n",
      "Epoch 106/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6565 - accuracy: 0.5612 - val_loss: 0.6527 - val_accuracy: 0.5797\n",
      "Epoch 107/500\n",
      "60/60 [==============================] - 4s 70ms/step - loss: 0.6538 - accuracy: 0.5601 - val_loss: 0.6549 - val_accuracy: 0.5930\n",
      "Epoch 108/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6504 - accuracy: 0.5625 - val_loss: 0.6584 - val_accuracy: 0.5831\n",
      "Epoch 109/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6542 - accuracy: 0.5549 - val_loss: 0.6575 - val_accuracy: 0.5980\n",
      "Epoch 110/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6465 - accuracy: 0.5766 - val_loss: 0.6580 - val_accuracy: 0.5864\n",
      "Epoch 111/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6594 - accuracy: 0.5664 - val_loss: 0.6557 - val_accuracy: 0.5814\n",
      "Epoch 112/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6566 - accuracy: 0.5630 - val_loss: 0.6587 - val_accuracy: 0.5847\n",
      "Epoch 113/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6509 - accuracy: 0.5722 - val_loss: 0.6573 - val_accuracy: 0.5831\n",
      "Epoch 114/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6534 - accuracy: 0.5619 - val_loss: 0.6584 - val_accuracy: 0.5880\n",
      "Epoch 115/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6501 - accuracy: 0.5761 - val_loss: 0.6601 - val_accuracy: 0.5847\n",
      "Epoch 116/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6551 - accuracy: 0.5761 - val_loss: 0.6616 - val_accuracy: 0.5814\n",
      "Epoch 117/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6528 - accuracy: 0.5648 - val_loss: 0.6575 - val_accuracy: 0.5814\n",
      "Epoch 118/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6562 - accuracy: 0.5601 - val_loss: 0.6572 - val_accuracy: 0.5748\n",
      "Epoch 119/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6502 - accuracy: 0.5790 - val_loss: 0.6638 - val_accuracy: 0.5797\n",
      "Epoch 120/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6564 - accuracy: 0.5659 - val_loss: 0.6629 - val_accuracy: 0.5781\n",
      "Epoch 121/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6498 - accuracy: 0.5654 - val_loss: 0.6676 - val_accuracy: 0.5814\n",
      "Epoch 122/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6510 - accuracy: 0.5709 - val_loss: 0.6691 - val_accuracy: 0.5980\n",
      "Epoch 123/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6508 - accuracy: 0.5675 - val_loss: 0.6641 - val_accuracy: 0.5914\n",
      "Epoch 124/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6550 - accuracy: 0.5690 - val_loss: 0.6582 - val_accuracy: 0.5947\n",
      "Epoch 125/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6536 - accuracy: 0.5706 - val_loss: 0.6642 - val_accuracy: 0.5797\n",
      "Epoch 126/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6586 - accuracy: 0.5625 - val_loss: 0.6621 - val_accuracy: 0.5681\n",
      "Epoch 127/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6527 - accuracy: 0.5690 - val_loss: 0.6591 - val_accuracy: 0.5714\n",
      "Epoch 128/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6508 - accuracy: 0.5606 - val_loss: 0.6568 - val_accuracy: 0.5864\n",
      "Epoch 129/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6535 - accuracy: 0.5559 - val_loss: 0.6607 - val_accuracy: 0.5847\n",
      "Epoch 130/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6471 - accuracy: 0.5703 - val_loss: 0.6566 - val_accuracy: 0.5797\n",
      "Epoch 131/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6505 - accuracy: 0.5711 - val_loss: 0.6580 - val_accuracy: 0.5947\n",
      "Epoch 132/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6479 - accuracy: 0.5748 - val_loss: 0.6662 - val_accuracy: 0.5764\n",
      "Epoch 133/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6523 - accuracy: 0.5714 - val_loss: 0.6656 - val_accuracy: 0.5814\n",
      "Epoch 134/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6449 - accuracy: 0.5803 - val_loss: 0.6660 - val_accuracy: 0.5714\n",
      "Epoch 135/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6532 - accuracy: 0.5711 - val_loss: 0.6640 - val_accuracy: 0.5864\n",
      "Epoch 136/500\n",
      "60/60 [==============================] - 4s 62ms/step - loss: 0.6491 - accuracy: 0.5748 - val_loss: 0.6691 - val_accuracy: 0.5748\n",
      "Epoch 137/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6509 - accuracy: 0.5732 - val_loss: 0.6607 - val_accuracy: 0.5930\n",
      "Epoch 138/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6518 - accuracy: 0.5714 - val_loss: 0.6651 - val_accuracy: 0.5781\n",
      "Epoch 139/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6500 - accuracy: 0.5774 - val_loss: 0.6694 - val_accuracy: 0.5449\n",
      "Epoch 140/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6506 - accuracy: 0.5745 - val_loss: 0.6658 - val_accuracy: 0.5681\n",
      "Epoch 141/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6529 - accuracy: 0.5612 - val_loss: 0.6722 - val_accuracy: 0.5482\n",
      "Epoch 142/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6469 - accuracy: 0.5780 - val_loss: 0.6687 - val_accuracy: 0.5598\n",
      "Epoch 143/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6466 - accuracy: 0.5835 - val_loss: 0.6708 - val_accuracy: 0.5748\n",
      "Epoch 144/500\n",
      "60/60 [==============================] - 4s 62ms/step - loss: 0.6497 - accuracy: 0.5638 - val_loss: 0.6722 - val_accuracy: 0.5581\n",
      "Epoch 145/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6506 - accuracy: 0.5769 - val_loss: 0.6630 - val_accuracy: 0.5731\n",
      "Epoch 146/500\n",
      "60/60 [==============================] - 4s 71ms/step - loss: 0.6488 - accuracy: 0.5735 - val_loss: 0.6650 - val_accuracy: 0.5797\n",
      "Epoch 147/500\n",
      "60/60 [==============================] - 4s 60ms/step - loss: 0.6456 - accuracy: 0.5753 - val_loss: 0.6757 - val_accuracy: 0.5648\n",
      "Epoch 148/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6497 - accuracy: 0.5787 - val_loss: 0.6767 - val_accuracy: 0.5698\n",
      "Epoch 149/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6503 - accuracy: 0.5606 - val_loss: 0.6831 - val_accuracy: 0.5432\n",
      "Epoch 150/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6485 - accuracy: 0.5722 - val_loss: 0.6818 - val_accuracy: 0.5615\n",
      "Epoch 151/500\n",
      "60/60 [==============================] - 4s 70ms/step - loss: 0.6487 - accuracy: 0.5814 - val_loss: 0.6890 - val_accuracy: 0.5598\n",
      "Epoch 152/500\n",
      "60/60 [==============================] - 4s 62ms/step - loss: 0.6460 - accuracy: 0.5711 - val_loss: 0.6823 - val_accuracy: 0.5615\n",
      "Epoch 153/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6461 - accuracy: 0.5829 - val_loss: 0.6901 - val_accuracy: 0.5714\n",
      "Epoch 154/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6484 - accuracy: 0.5727 - val_loss: 0.6865 - val_accuracy: 0.5565\n",
      "Epoch 155/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6493 - accuracy: 0.5654 - val_loss: 0.6790 - val_accuracy: 0.5664\n",
      "Epoch 156/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6522 - accuracy: 0.5604 - val_loss: 0.6731 - val_accuracy: 0.5681\n",
      "Epoch 157/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6540 - accuracy: 0.5738 - val_loss: 0.6716 - val_accuracy: 0.5764\n",
      "Epoch 158/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6514 - accuracy: 0.5709 - val_loss: 0.6739 - val_accuracy: 0.5615\n",
      "Epoch 159/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6474 - accuracy: 0.5759 - val_loss: 0.6743 - val_accuracy: 0.5731\n",
      "Epoch 160/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6475 - accuracy: 0.5780 - val_loss: 0.6787 - val_accuracy: 0.5631\n",
      "Epoch 161/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6502 - accuracy: 0.5598 - val_loss: 0.6739 - val_accuracy: 0.5598\n",
      "Epoch 162/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6510 - accuracy: 0.5706 - val_loss: 0.6722 - val_accuracy: 0.5731\n",
      "Epoch 163/500\n",
      "60/60 [==============================] - 4s 69ms/step - loss: 0.6462 - accuracy: 0.5822 - val_loss: 0.6717 - val_accuracy: 0.5648\n",
      "Epoch 164/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6557 - accuracy: 0.5593 - val_loss: 0.6740 - val_accuracy: 0.5631\n",
      "Epoch 165/500\n",
      "60/60 [==============================] - 5s 75ms/step - loss: 0.6456 - accuracy: 0.5711 - val_loss: 0.6748 - val_accuracy: 0.5598\n",
      "Epoch 166/500\n",
      "60/60 [==============================] - 4s 71ms/step - loss: 0.6494 - accuracy: 0.5706 - val_loss: 0.6771 - val_accuracy: 0.5532\n",
      "Epoch 167/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6477 - accuracy: 0.5740 - val_loss: 0.6736 - val_accuracy: 0.5698\n",
      "Epoch 168/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6490 - accuracy: 0.5706 - val_loss: 0.6731 - val_accuracy: 0.5648\n",
      "Epoch 169/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6469 - accuracy: 0.5780 - val_loss: 0.6702 - val_accuracy: 0.5781\n",
      "Epoch 170/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6456 - accuracy: 0.5740 - val_loss: 0.6692 - val_accuracy: 0.5681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6481 - accuracy: 0.5719 - val_loss: 0.6727 - val_accuracy: 0.5648\n",
      "Epoch 172/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6458 - accuracy: 0.5751 - val_loss: 0.6682 - val_accuracy: 0.5831\n",
      "Epoch 173/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6505 - accuracy: 0.5682 - val_loss: 0.6690 - val_accuracy: 0.5731\n",
      "Epoch 174/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6437 - accuracy: 0.5675 - val_loss: 0.6655 - val_accuracy: 0.5781\n",
      "Epoch 175/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6435 - accuracy: 0.5711 - val_loss: 0.6701 - val_accuracy: 0.5664\n",
      "Epoch 176/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6494 - accuracy: 0.5622 - val_loss: 0.6642 - val_accuracy: 0.5764\n",
      "Epoch 177/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6490 - accuracy: 0.5659 - val_loss: 0.6685 - val_accuracy: 0.5714\n",
      "Epoch 178/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6463 - accuracy: 0.5753 - val_loss: 0.6688 - val_accuracy: 0.5764\n",
      "Epoch 179/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6429 - accuracy: 0.5900 - val_loss: 0.6666 - val_accuracy: 0.5781\n",
      "Epoch 180/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6470 - accuracy: 0.5745 - val_loss: 0.6731 - val_accuracy: 0.5714\n",
      "Epoch 181/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6488 - accuracy: 0.5827 - val_loss: 0.6753 - val_accuracy: 0.5615\n",
      "Epoch 182/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6455 - accuracy: 0.5787 - val_loss: 0.6718 - val_accuracy: 0.5781\n",
      "Epoch 183/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6449 - accuracy: 0.5814 - val_loss: 0.6783 - val_accuracy: 0.5797\n",
      "Epoch 184/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6462 - accuracy: 0.5764 - val_loss: 0.6805 - val_accuracy: 0.5598\n",
      "Epoch 185/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6442 - accuracy: 0.5882 - val_loss: 0.6817 - val_accuracy: 0.5548\n",
      "Epoch 186/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6459 - accuracy: 0.5774 - val_loss: 0.6769 - val_accuracy: 0.5615\n",
      "Epoch 187/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6475 - accuracy: 0.5772 - val_loss: 0.6761 - val_accuracy: 0.5598\n",
      "Epoch 188/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6454 - accuracy: 0.5932 - val_loss: 0.6725 - val_accuracy: 0.5648\n",
      "Epoch 189/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6438 - accuracy: 0.5885 - val_loss: 0.6775 - val_accuracy: 0.5664\n",
      "Epoch 190/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6450 - accuracy: 0.5727 - val_loss: 0.6725 - val_accuracy: 0.5781\n",
      "Epoch 191/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6421 - accuracy: 0.5832 - val_loss: 0.6785 - val_accuracy: 0.5648\n",
      "Epoch 192/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6462 - accuracy: 0.5798 - val_loss: 0.6720 - val_accuracy: 0.5814\n",
      "Epoch 193/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6463 - accuracy: 0.5795 - val_loss: 0.6740 - val_accuracy: 0.5598\n",
      "Epoch 194/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6437 - accuracy: 0.5787 - val_loss: 0.6769 - val_accuracy: 0.5648\n",
      "Epoch 195/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6456 - accuracy: 0.5845 - val_loss: 0.6857 - val_accuracy: 0.5498\n",
      "Epoch 196/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6446 - accuracy: 0.5853 - val_loss: 0.6778 - val_accuracy: 0.5548\n",
      "Epoch 197/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6471 - accuracy: 0.5714 - val_loss: 0.6776 - val_accuracy: 0.5548\n",
      "Epoch 198/500\n",
      "60/60 [==============================] - 4s 69ms/step - loss: 0.6404 - accuracy: 0.5798 - val_loss: 0.6748 - val_accuracy: 0.5615\n",
      "Epoch 199/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6413 - accuracy: 0.5866 - val_loss: 0.6760 - val_accuracy: 0.5581\n",
      "Epoch 200/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6461 - accuracy: 0.5719 - val_loss: 0.6768 - val_accuracy: 0.5565\n",
      "Epoch 201/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6469 - accuracy: 0.5832 - val_loss: 0.6839 - val_accuracy: 0.5482\n",
      "Epoch 202/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6418 - accuracy: 0.5853 - val_loss: 0.6799 - val_accuracy: 0.5565\n",
      "Epoch 203/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6426 - accuracy: 0.5777 - val_loss: 0.6831 - val_accuracy: 0.5548\n",
      "Epoch 204/500\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.6437 - accuracy: 0.5795 - val_loss: 0.6806 - val_accuracy: 0.5565\n",
      "Epoch 205/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6428 - accuracy: 0.5787 - val_loss: 0.6833 - val_accuracy: 0.5532\n",
      "Epoch 206/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6445 - accuracy: 0.5816 - val_loss: 0.6835 - val_accuracy: 0.5565\n",
      "Epoch 207/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6450 - accuracy: 0.5774 - val_loss: 0.6879 - val_accuracy: 0.5532\n",
      "Epoch 208/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6440 - accuracy: 0.5793 - val_loss: 0.7055 - val_accuracy: 0.5449\n",
      "Epoch 209/500\n",
      "60/60 [==============================] - 4s 74ms/step - loss: 0.6439 - accuracy: 0.5866 - val_loss: 0.6824 - val_accuracy: 0.5581\n",
      "Epoch 210/500\n",
      "60/60 [==============================] - 4s 69ms/step - loss: 0.6464 - accuracy: 0.5753 - val_loss: 0.6800 - val_accuracy: 0.5631\n",
      "Epoch 211/500\n",
      "60/60 [==============================] - 4s 70ms/step - loss: 0.6458 - accuracy: 0.5774 - val_loss: 0.6824 - val_accuracy: 0.5681\n",
      "Epoch 212/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6476 - accuracy: 0.5693 - val_loss: 0.6885 - val_accuracy: 0.5648\n",
      "Epoch 213/500\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.6474 - accuracy: 0.5782 - val_loss: 0.6853 - val_accuracy: 0.5731\n",
      "Epoch 214/500\n",
      "60/60 [==============================] - 4s 70ms/step - loss: 0.6453 - accuracy: 0.5717 - val_loss: 0.6825 - val_accuracy: 0.5681\n",
      "Epoch 215/500\n",
      "60/60 [==============================] - 5s 84ms/step - loss: 0.6427 - accuracy: 0.5743 - val_loss: 0.6789 - val_accuracy: 0.5764\n",
      "Epoch 216/500\n",
      "60/60 [==============================] - 5s 80ms/step - loss: 0.6475 - accuracy: 0.5769 - val_loss: 0.6767 - val_accuracy: 0.5598\n",
      "Epoch 217/500\n",
      "60/60 [==============================] - 5s 75ms/step - loss: 0.6445 - accuracy: 0.5785 - val_loss: 0.6780 - val_accuracy: 0.5664\n",
      "Epoch 218/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6433 - accuracy: 0.5787 - val_loss: 0.6776 - val_accuracy: 0.5615\n",
      "Epoch 219/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6442 - accuracy: 0.5808 - val_loss: 0.6765 - val_accuracy: 0.5581\n",
      "Epoch 220/500\n",
      "60/60 [==============================] - 4s 72ms/step - loss: 0.6420 - accuracy: 0.5827 - val_loss: 0.6824 - val_accuracy: 0.5681\n",
      "Epoch 221/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6451 - accuracy: 0.5780 - val_loss: 0.6896 - val_accuracy: 0.5598\n",
      "Epoch 222/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6431 - accuracy: 0.5829 - val_loss: 0.6823 - val_accuracy: 0.5631\n",
      "Epoch 223/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6460 - accuracy: 0.5769 - val_loss: 0.6861 - val_accuracy: 0.5565\n",
      "Epoch 224/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6437 - accuracy: 0.5766 - val_loss: 0.6754 - val_accuracy: 0.5681\n",
      "Epoch 225/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6438 - accuracy: 0.5835 - val_loss: 0.6700 - val_accuracy: 0.5847\n",
      "Epoch 226/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6388 - accuracy: 0.5892 - val_loss: 0.6867 - val_accuracy: 0.5432\n",
      "Epoch 227/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6461 - accuracy: 0.5816 - val_loss: 0.6868 - val_accuracy: 0.5515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 228/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6450 - accuracy: 0.5735 - val_loss: 0.6783 - val_accuracy: 0.5631\n",
      "Epoch 229/500\n",
      "60/60 [==============================] - 4s 70ms/step - loss: 0.6466 - accuracy: 0.5609 - val_loss: 0.6880 - val_accuracy: 0.5498\n",
      "Epoch 230/500\n",
      "60/60 [==============================] - 5s 78ms/step - loss: 0.6431 - accuracy: 0.5887 - val_loss: 0.6950 - val_accuracy: 0.5515\n",
      "Epoch 231/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6454 - accuracy: 0.5822 - val_loss: 0.6790 - val_accuracy: 0.5565\n",
      "Epoch 232/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6423 - accuracy: 0.5827 - val_loss: 0.6806 - val_accuracy: 0.5548\n",
      "Epoch 233/500\n",
      "60/60 [==============================] - 4s 62ms/step - loss: 0.6414 - accuracy: 0.5806 - val_loss: 0.6815 - val_accuracy: 0.5249\n",
      "Epoch 234/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6447 - accuracy: 0.5837 - val_loss: 0.6756 - val_accuracy: 0.5199\n",
      "Epoch 235/500\n",
      "60/60 [==============================] - 4s 69ms/step - loss: 0.6446 - accuracy: 0.5835 - val_loss: 0.6791 - val_accuracy: 0.5465\n",
      "Epoch 236/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6406 - accuracy: 0.5903 - val_loss: 0.6834 - val_accuracy: 0.5565\n",
      "Epoch 237/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6400 - accuracy: 0.5761 - val_loss: 0.6837 - val_accuracy: 0.5532\n",
      "Epoch 238/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6449 - accuracy: 0.5793 - val_loss: 0.6787 - val_accuracy: 0.5581\n",
      "Epoch 239/500\n",
      "60/60 [==============================] - 4s 70ms/step - loss: 0.6436 - accuracy: 0.5769 - val_loss: 0.6803 - val_accuracy: 0.5631\n",
      "Epoch 240/500\n",
      "60/60 [==============================] - 4s 69ms/step - loss: 0.6428 - accuracy: 0.5811 - val_loss: 0.6933 - val_accuracy: 0.5449\n",
      "Epoch 241/500\n",
      "60/60 [==============================] - 4s 72ms/step - loss: 0.6428 - accuracy: 0.5861 - val_loss: 0.6758 - val_accuracy: 0.5648\n",
      "Epoch 242/500\n",
      "60/60 [==============================] - 4s 70ms/step - loss: 0.6478 - accuracy: 0.5837 - val_loss: 0.6831 - val_accuracy: 0.5698\n",
      "Epoch 243/500\n",
      "60/60 [==============================] - 4s 71ms/step - loss: 0.6449 - accuracy: 0.5759 - val_loss: 0.6880 - val_accuracy: 0.5565\n",
      "Epoch 244/500\n",
      "60/60 [==============================] - 5s 75ms/step - loss: 0.6481 - accuracy: 0.5619 - val_loss: 0.7061 - val_accuracy: 0.5449\n",
      "Epoch 245/500\n",
      "60/60 [==============================] - 4s 71ms/step - loss: 0.6429 - accuracy: 0.5869 - val_loss: 0.6934 - val_accuracy: 0.5449\n",
      "Epoch 246/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6443 - accuracy: 0.5748 - val_loss: 0.6984 - val_accuracy: 0.5233\n",
      "Epoch 247/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6413 - accuracy: 0.5866 - val_loss: 0.6875 - val_accuracy: 0.5565\n",
      "Epoch 248/500\n",
      "60/60 [==============================] - 4s 72ms/step - loss: 0.6419 - accuracy: 0.5866 - val_loss: 0.6841 - val_accuracy: 0.5548\n",
      "Epoch 249/500\n",
      "60/60 [==============================] - 4s 74ms/step - loss: 0.6423 - accuracy: 0.5803 - val_loss: 0.7021 - val_accuracy: 0.5415\n",
      "Epoch 250/500\n",
      "60/60 [==============================] - 4s 72ms/step - loss: 0.6466 - accuracy: 0.5761 - val_loss: 0.6801 - val_accuracy: 0.5631\n",
      "Epoch 251/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6384 - accuracy: 0.5969 - val_loss: 0.6775 - val_accuracy: 0.5581\n",
      "Epoch 252/500\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.6394 - accuracy: 0.5916 - val_loss: 0.6897 - val_accuracy: 0.5498\n",
      "Epoch 253/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6414 - accuracy: 0.5864 - val_loss: 0.6808 - val_accuracy: 0.5548\n",
      "Epoch 254/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6428 - accuracy: 0.5803 - val_loss: 0.6918 - val_accuracy: 0.5482\n",
      "Epoch 255/500\n",
      "60/60 [==============================] - 4s 69ms/step - loss: 0.6434 - accuracy: 0.5803 - val_loss: 0.6887 - val_accuracy: 0.5581\n",
      "Epoch 256/500\n",
      "60/60 [==============================] - 4s 73ms/step - loss: 0.6413 - accuracy: 0.5850 - val_loss: 0.6853 - val_accuracy: 0.5631\n",
      "Epoch 257/500\n",
      "60/60 [==============================] - 5s 88ms/step - loss: 0.6420 - accuracy: 0.5840 - val_loss: 0.6784 - val_accuracy: 0.5631\n",
      "Epoch 258/500\n",
      "60/60 [==============================] - 6s 92ms/step - loss: 0.6425 - accuracy: 0.5756 - val_loss: 0.6884 - val_accuracy: 0.5482\n",
      "Epoch 259/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6440 - accuracy: 0.5824 - val_loss: 0.6860 - val_accuracy: 0.5482\n",
      "Epoch 260/500\n",
      "60/60 [==============================] - 5s 89ms/step - loss: 0.6420 - accuracy: 0.5824 - val_loss: 0.6824 - val_accuracy: 0.5565\n",
      "Epoch 261/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6392 - accuracy: 0.5814 - val_loss: 0.6745 - val_accuracy: 0.5648\n",
      "Epoch 262/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6443 - accuracy: 0.5724 - val_loss: 0.6725 - val_accuracy: 0.5714\n",
      "Epoch 263/500\n",
      "60/60 [==============================] - 4s 62ms/step - loss: 0.6403 - accuracy: 0.5885 - val_loss: 0.6832 - val_accuracy: 0.5532\n",
      "Epoch 264/500\n",
      "60/60 [==============================] - 4s 72ms/step - loss: 0.6424 - accuracy: 0.5793 - val_loss: 0.6847 - val_accuracy: 0.5565\n",
      "Epoch 265/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6457 - accuracy: 0.5751 - val_loss: 0.6775 - val_accuracy: 0.5698\n",
      "Epoch 266/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6432 - accuracy: 0.5832 - val_loss: 0.6683 - val_accuracy: 0.5681\n",
      "Epoch 267/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6430 - accuracy: 0.5814 - val_loss: 0.6752 - val_accuracy: 0.5797\n",
      "Epoch 268/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6409 - accuracy: 0.5940 - val_loss: 0.6811 - val_accuracy: 0.5664\n",
      "Epoch 269/500\n",
      "60/60 [==============================] - 4s 71ms/step - loss: 0.6406 - accuracy: 0.5971 - val_loss: 0.6847 - val_accuracy: 0.5615\n",
      "Epoch 270/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6440 - accuracy: 0.5751 - val_loss: 0.6915 - val_accuracy: 0.5548\n",
      "Epoch 271/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6419 - accuracy: 0.5814 - val_loss: 0.6940 - val_accuracy: 0.5581\n",
      "Epoch 272/500\n",
      "60/60 [==============================] - 4s 69ms/step - loss: 0.6405 - accuracy: 0.5774 - val_loss: 0.7050 - val_accuracy: 0.5365\n",
      "Epoch 273/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6435 - accuracy: 0.5753 - val_loss: 0.6964 - val_accuracy: 0.5565\n",
      "Epoch 274/500\n",
      "60/60 [==============================] - 4s 73ms/step - loss: 0.6414 - accuracy: 0.5858 - val_loss: 0.6937 - val_accuracy: 0.5465\n",
      "Epoch 275/500\n",
      "60/60 [==============================] - 5s 83ms/step - loss: 0.6414 - accuracy: 0.5850 - val_loss: 0.6915 - val_accuracy: 0.5532\n",
      "Epoch 276/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6450 - accuracy: 0.5727 - val_loss: 0.6921 - val_accuracy: 0.5615\n",
      "Epoch 277/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6406 - accuracy: 0.5777 - val_loss: 0.6767 - val_accuracy: 0.5731\n",
      "Epoch 278/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6430 - accuracy: 0.5819 - val_loss: 0.6699 - val_accuracy: 0.5880\n",
      "Epoch 279/500\n",
      "60/60 [==============================] - 4s 71ms/step - loss: 0.6415 - accuracy: 0.5777 - val_loss: 0.6757 - val_accuracy: 0.5648\n",
      "Epoch 280/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6410 - accuracy: 0.5869 - val_loss: 0.6834 - val_accuracy: 0.5498\n",
      "Epoch 281/500\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.6415 - accuracy: 0.5829 - val_loss: 0.6743 - val_accuracy: 0.5598\n",
      "Epoch 282/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6410 - accuracy: 0.5866 - val_loss: 0.6701 - val_accuracy: 0.5681\n",
      "Epoch 283/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6436 - accuracy: 0.5717 - val_loss: 0.6801 - val_accuracy: 0.5615\n",
      "Epoch 284/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6417 - accuracy: 0.5798 - val_loss: 0.6730 - val_accuracy: 0.5698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 285/500\n",
      "60/60 [==============================] - 4s 62ms/step - loss: 0.6387 - accuracy: 0.5911 - val_loss: 0.6710 - val_accuracy: 0.5664\n",
      "Epoch 286/500\n",
      "60/60 [==============================] - 4s 62ms/step - loss: 0.6400 - accuracy: 0.5824 - val_loss: 0.6749 - val_accuracy: 0.5581\n",
      "Epoch 287/500\n",
      "60/60 [==============================] - 4s 72ms/step - loss: 0.6421 - accuracy: 0.5856 - val_loss: 0.6739 - val_accuracy: 0.5631\n",
      "Epoch 288/500\n",
      "60/60 [==============================] - 4s 59ms/step - loss: 0.6422 - accuracy: 0.5806 - val_loss: 0.6846 - val_accuracy: 0.5548\n",
      "Epoch 289/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6421 - accuracy: 0.5777 - val_loss: 0.6915 - val_accuracy: 0.5515\n",
      "Epoch 290/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6385 - accuracy: 0.5913 - val_loss: 0.7003 - val_accuracy: 0.5349\n",
      "Epoch 291/500\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.6416 - accuracy: 0.5832 - val_loss: 0.6806 - val_accuracy: 0.5664\n",
      "Epoch 292/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6379 - accuracy: 0.5845 - val_loss: 0.7145 - val_accuracy: 0.5266\n",
      "Epoch 293/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6414 - accuracy: 0.5871 - val_loss: 0.6694 - val_accuracy: 0.5997\n",
      "Epoch 294/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6448 - accuracy: 0.5688 - val_loss: 0.6756 - val_accuracy: 0.5598\n",
      "Epoch 295/500\n",
      "60/60 [==============================] - 4s 61ms/step - loss: 0.6393 - accuracy: 0.5892 - val_loss: 0.6827 - val_accuracy: 0.5581\n",
      "Epoch 296/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6399 - accuracy: 0.5879 - val_loss: 0.6874 - val_accuracy: 0.5548\n",
      "Epoch 297/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6383 - accuracy: 0.5958 - val_loss: 0.6888 - val_accuracy: 0.5415\n",
      "Epoch 298/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6407 - accuracy: 0.5832 - val_loss: 0.6866 - val_accuracy: 0.5548\n",
      "Epoch 299/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6444 - accuracy: 0.5748 - val_loss: 0.6812 - val_accuracy: 0.5664\n",
      "Epoch 300/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6408 - accuracy: 0.5822 - val_loss: 0.6810 - val_accuracy: 0.5648\n",
      "Epoch 301/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6435 - accuracy: 0.5827 - val_loss: 0.6733 - val_accuracy: 0.5482\n",
      "Epoch 302/500\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.6388 - accuracy: 0.5934 - val_loss: 0.6734 - val_accuracy: 0.5797\n",
      "Epoch 303/500\n",
      "60/60 [==============================] - 4s 61ms/step - loss: 0.6448 - accuracy: 0.5793 - val_loss: 0.6786 - val_accuracy: 0.5615\n",
      "Epoch 304/500\n",
      "60/60 [==============================] - 4s 58ms/step - loss: 0.6400 - accuracy: 0.5911 - val_loss: 0.6887 - val_accuracy: 0.5515\n",
      "Epoch 305/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6428 - accuracy: 0.5769 - val_loss: 0.6809 - val_accuracy: 0.5698\n",
      "Epoch 306/500\n",
      "60/60 [==============================] - 4s 59ms/step - loss: 0.6393 - accuracy: 0.5853 - val_loss: 0.6749 - val_accuracy: 0.5698\n",
      "Epoch 307/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6423 - accuracy: 0.5811 - val_loss: 0.6737 - val_accuracy: 0.5664\n",
      "Epoch 308/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6370 - accuracy: 0.5871 - val_loss: 0.6818 - val_accuracy: 0.5565\n",
      "Epoch 309/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6385 - accuracy: 0.5908 - val_loss: 0.6902 - val_accuracy: 0.5532\n",
      "Epoch 310/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6440 - accuracy: 0.5756 - val_loss: 0.6739 - val_accuracy: 0.5797\n",
      "Epoch 311/500\n",
      "60/60 [==============================] - 3s 58ms/step - loss: 0.6435 - accuracy: 0.5808 - val_loss: 0.6720 - val_accuracy: 0.5764\n",
      "Epoch 312/500\n",
      "60/60 [==============================] - 4s 61ms/step - loss: 0.6412 - accuracy: 0.5811 - val_loss: 0.6733 - val_accuracy: 0.5814\n",
      "Epoch 313/500\n",
      "60/60 [==============================] - 3s 58ms/step - loss: 0.6376 - accuracy: 0.5913 - val_loss: 0.6775 - val_accuracy: 0.5698\n",
      "Epoch 314/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6386 - accuracy: 0.5950 - val_loss: 0.6757 - val_accuracy: 0.5664\n",
      "Epoch 315/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6409 - accuracy: 0.5864 - val_loss: 0.6850 - val_accuracy: 0.5581\n",
      "Epoch 316/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6424 - accuracy: 0.5798 - val_loss: 0.6808 - val_accuracy: 0.5631\n",
      "Epoch 317/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6386 - accuracy: 0.5898 - val_loss: 0.6681 - val_accuracy: 0.5814\n",
      "Epoch 318/500\n",
      "60/60 [==============================] - 3s 58ms/step - loss: 0.6411 - accuracy: 0.5890 - val_loss: 0.6671 - val_accuracy: 0.5831\n",
      "Epoch 319/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6408 - accuracy: 0.5832 - val_loss: 0.6631 - val_accuracy: 0.5831\n",
      "Epoch 320/500\n",
      "60/60 [==============================] - 4s 61ms/step - loss: 0.6418 - accuracy: 0.5885 - val_loss: 0.6757 - val_accuracy: 0.5831\n",
      "Epoch 321/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6397 - accuracy: 0.5832 - val_loss: 0.6760 - val_accuracy: 0.5797\n",
      "Epoch 322/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6423 - accuracy: 0.5850 - val_loss: 0.6810 - val_accuracy: 0.5615\n",
      "Epoch 323/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6406 - accuracy: 0.5900 - val_loss: 0.6808 - val_accuracy: 0.5664\n",
      "Epoch 324/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6427 - accuracy: 0.5801 - val_loss: 0.6840 - val_accuracy: 0.5631\n",
      "Epoch 325/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6384 - accuracy: 0.5953 - val_loss: 0.6832 - val_accuracy: 0.5615\n",
      "Epoch 326/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6391 - accuracy: 0.5861 - val_loss: 0.6869 - val_accuracy: 0.5532\n",
      "Epoch 327/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6399 - accuracy: 0.5853 - val_loss: 0.6796 - val_accuracy: 0.5664\n",
      "Epoch 328/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6395 - accuracy: 0.5845 - val_loss: 0.6780 - val_accuracy: 0.5648\n",
      "Epoch 329/500\n",
      "60/60 [==============================] - 4s 61ms/step - loss: 0.6411 - accuracy: 0.5843 - val_loss: 0.6760 - val_accuracy: 0.5698\n",
      "Epoch 330/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6365 - accuracy: 0.5882 - val_loss: 0.6797 - val_accuracy: 0.5365\n",
      "Epoch 331/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6406 - accuracy: 0.5840 - val_loss: 0.6734 - val_accuracy: 0.5648\n",
      "Epoch 332/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6393 - accuracy: 0.5916 - val_loss: 0.6690 - val_accuracy: 0.5714\n",
      "Epoch 333/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6422 - accuracy: 0.5887 - val_loss: 0.6783 - val_accuracy: 0.5714\n",
      "Epoch 334/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6396 - accuracy: 0.5963 - val_loss: 0.6744 - val_accuracy: 0.5698\n",
      "Epoch 335/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6399 - accuracy: 0.5916 - val_loss: 0.6785 - val_accuracy: 0.5615\n",
      "Epoch 336/500\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6389 - accuracy: 0.5869 - val_loss: 0.6879 - val_accuracy: 0.5532\n",
      "Epoch 337/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6418 - accuracy: 0.5853 - val_loss: 0.6882 - val_accuracy: 0.5565\n",
      "Epoch 338/500\n",
      "60/60 [==============================] - 4s 72ms/step - loss: 0.6436 - accuracy: 0.5711 - val_loss: 0.6855 - val_accuracy: 0.5615\n",
      "Epoch 339/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6373 - accuracy: 0.5927 - val_loss: 0.6756 - val_accuracy: 0.5814\n",
      "Epoch 340/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6393 - accuracy: 0.5837 - val_loss: 0.6740 - val_accuracy: 0.5731\n",
      "Epoch 341/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6420 - accuracy: 0.5837 - val_loss: 0.6770 - val_accuracy: 0.5714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342/500\n",
      "60/60 [==============================] - 4s 62ms/step - loss: 0.6369 - accuracy: 0.5995 - val_loss: 0.6844 - val_accuracy: 0.5598\n",
      "Epoch 343/500\n",
      "60/60 [==============================] - 4s 62ms/step - loss: 0.6411 - accuracy: 0.5850 - val_loss: 0.6751 - val_accuracy: 0.5880\n",
      "Epoch 344/500\n",
      "60/60 [==============================] - 4s 62ms/step - loss: 0.6372 - accuracy: 0.5992 - val_loss: 0.6770 - val_accuracy: 0.5831\n",
      "Epoch 345/500\n",
      "60/60 [==============================] - 5s 76ms/step - loss: 0.6385 - accuracy: 0.5929 - val_loss: 0.6772 - val_accuracy: 0.5814\n",
      "Epoch 346/500\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.6392 - accuracy: 0.5966 - val_loss: 0.6812 - val_accuracy: 0.5615\n",
      "Epoch 347/500\n",
      "60/60 [==============================] - 4s 71ms/step - loss: 0.6414 - accuracy: 0.5832 - val_loss: 0.6780 - val_accuracy: 0.5631\n",
      "Epoch 348/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6392 - accuracy: 0.5961 - val_loss: 0.6834 - val_accuracy: 0.5631\n",
      "Epoch 349/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6411 - accuracy: 0.5816 - val_loss: 0.6927 - val_accuracy: 0.5664\n",
      "Epoch 350/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6353 - accuracy: 0.5937 - val_loss: 0.6979 - val_accuracy: 0.5316\n",
      "Epoch 351/500\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.6400 - accuracy: 0.5871 - val_loss: 0.6819 - val_accuracy: 0.5598\n",
      "Epoch 352/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6396 - accuracy: 0.5816 - val_loss: 0.6836 - val_accuracy: 0.5565\n",
      "Epoch 353/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6381 - accuracy: 0.5953 - val_loss: 0.6842 - val_accuracy: 0.5548\n",
      "Epoch 354/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6385 - accuracy: 0.5890 - val_loss: 0.6913 - val_accuracy: 0.5382\n",
      "Epoch 355/500\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.6361 - accuracy: 0.5898 - val_loss: 0.6683 - val_accuracy: 0.5698\n",
      "Epoch 356/500\n",
      "60/60 [==============================] - 4s 73ms/step - loss: 0.6410 - accuracy: 0.5848 - val_loss: 0.6727 - val_accuracy: 0.5714\n",
      "Epoch 357/500\n",
      "60/60 [==============================] - 5s 76ms/step - loss: 0.6396 - accuracy: 0.5942 - val_loss: 0.6845 - val_accuracy: 0.5648\n",
      "Epoch 358/500\n",
      "60/60 [==============================] - 4s 73ms/step - loss: 0.6396 - accuracy: 0.5871 - val_loss: 0.6767 - val_accuracy: 0.5731\n",
      "Epoch 359/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6370 - accuracy: 0.5958 - val_loss: 0.6692 - val_accuracy: 0.5847\n",
      "Epoch 360/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6422 - accuracy: 0.5837 - val_loss: 0.6928 - val_accuracy: 0.5432\n",
      "Epoch 361/500\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.6401 - accuracy: 0.5827 - val_loss: 0.6838 - val_accuracy: 0.5615\n",
      "Epoch 362/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6369 - accuracy: 0.5898 - val_loss: 0.6918 - val_accuracy: 0.5465\n",
      "Epoch 363/500\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.6384 - accuracy: 0.5829 - val_loss: 0.6853 - val_accuracy: 0.5498\n",
      "Epoch 364/500\n",
      "60/60 [==============================] - 4s 69ms/step - loss: 0.6378 - accuracy: 0.5856 - val_loss: 0.6871 - val_accuracy: 0.5664\n",
      "Epoch 365/500\n",
      "60/60 [==============================] - 4s 69ms/step - loss: 0.6414 - accuracy: 0.5853 - val_loss: 0.6948 - val_accuracy: 0.5316\n",
      "Epoch 366/500\n",
      "60/60 [==============================] - 4s 69ms/step - loss: 0.6403 - accuracy: 0.5874 - val_loss: 0.7004 - val_accuracy: 0.5233\n",
      "Epoch 367/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6359 - accuracy: 0.5937 - val_loss: 0.6839 - val_accuracy: 0.5465\n",
      "Epoch 368/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6407 - accuracy: 0.5811 - val_loss: 0.6737 - val_accuracy: 0.5698\n",
      "Epoch 369/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6379 - accuracy: 0.5879 - val_loss: 0.6828 - val_accuracy: 0.5598\n",
      "Epoch 370/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6380 - accuracy: 0.5913 - val_loss: 0.6795 - val_accuracy: 0.5681\n",
      "Epoch 371/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6421 - accuracy: 0.5824 - val_loss: 0.6779 - val_accuracy: 0.5681\n",
      "Epoch 372/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6376 - accuracy: 0.5963 - val_loss: 0.6803 - val_accuracy: 0.5648\n",
      "Epoch 373/500\n",
      "60/60 [==============================] - 3s 55ms/step - loss: 0.6361 - accuracy: 0.6037 - val_loss: 0.6838 - val_accuracy: 0.5532\n",
      "Epoch 374/500\n",
      "60/60 [==============================] - 5s 89ms/step - loss: 0.6390 - accuracy: 0.5850 - val_loss: 0.6762 - val_accuracy: 0.5598\n",
      "Epoch 375/500\n",
      "60/60 [==============================] - 4s 71ms/step - loss: 0.6390 - accuracy: 0.5929 - val_loss: 0.6965 - val_accuracy: 0.5415\n",
      "Epoch 376/500\n",
      "60/60 [==============================] - 4s 73ms/step - loss: 0.6409 - accuracy: 0.5861 - val_loss: 0.6893 - val_accuracy: 0.5631\n",
      "Epoch 377/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6396 - accuracy: 0.5777 - val_loss: 0.6833 - val_accuracy: 0.5449\n",
      "Epoch 378/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6380 - accuracy: 0.5879 - val_loss: 0.6921 - val_accuracy: 0.5382\n",
      "Epoch 379/500\n",
      "60/60 [==============================] - 4s 75ms/step - loss: 0.6420 - accuracy: 0.5827 - val_loss: 0.6788 - val_accuracy: 0.5631\n",
      "Epoch 380/500\n",
      "60/60 [==============================] - 5s 79ms/step - loss: 0.6381 - accuracy: 0.5963 - val_loss: 0.6831 - val_accuracy: 0.5631\n",
      "Epoch 381/500\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 0.6392 - accuracy: 0.5887 - val_loss: 0.6892 - val_accuracy: 0.5631\n",
      "Epoch 382/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6357 - accuracy: 0.6013 - val_loss: 0.6884 - val_accuracy: 0.5698\n",
      "Epoch 383/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6410 - accuracy: 0.5845 - val_loss: 0.6855 - val_accuracy: 0.5648\n",
      "Epoch 384/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6394 - accuracy: 0.5934 - val_loss: 0.6779 - val_accuracy: 0.5598\n",
      "Epoch 385/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6406 - accuracy: 0.5890 - val_loss: 0.6731 - val_accuracy: 0.5847\n",
      "Epoch 386/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6372 - accuracy: 0.5843 - val_loss: 0.7309 - val_accuracy: 0.5299\n",
      "Epoch 387/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6387 - accuracy: 0.5887 - val_loss: 0.6965 - val_accuracy: 0.5415\n",
      "Epoch 388/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6415 - accuracy: 0.5869 - val_loss: 0.6669 - val_accuracy: 0.5814\n",
      "Epoch 389/500\n",
      "60/60 [==============================] - 5s 75ms/step - loss: 0.6397 - accuracy: 0.5837 - val_loss: 0.6681 - val_accuracy: 0.5897\n",
      "Epoch 390/500\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.6401 - accuracy: 0.5940 - val_loss: 0.6873 - val_accuracy: 0.5681\n",
      "Epoch 391/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6346 - accuracy: 0.5921 - val_loss: 0.7031 - val_accuracy: 0.5382\n",
      "Epoch 392/500\n",
      "60/60 [==============================] - 4s 69ms/step - loss: 0.6363 - accuracy: 0.5895 - val_loss: 0.6940 - val_accuracy: 0.5482\n",
      "Epoch 393/500\n",
      "60/60 [==============================] - 5s 78ms/step - loss: 0.6383 - accuracy: 0.5940 - val_loss: 0.6771 - val_accuracy: 0.5731\n",
      "Epoch 394/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6350 - accuracy: 0.6031 - val_loss: 0.6927 - val_accuracy: 0.5598\n",
      "Epoch 395/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6369 - accuracy: 0.5903 - val_loss: 0.6763 - val_accuracy: 0.5664\n",
      "Epoch 396/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6415 - accuracy: 0.5853 - val_loss: 0.6696 - val_accuracy: 0.5764\n",
      "Epoch 397/500\n",
      "60/60 [==============================] - 4s 71ms/step - loss: 0.6360 - accuracy: 0.5892 - val_loss: 0.6756 - val_accuracy: 0.5664\n",
      "Epoch 398/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6395 - accuracy: 0.5913 - val_loss: 0.6886 - val_accuracy: 0.5681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 399/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6408 - accuracy: 0.5879 - val_loss: 0.6715 - val_accuracy: 0.5797\n",
      "Epoch 400/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6359 - accuracy: 0.5953 - val_loss: 0.6755 - val_accuracy: 0.5814\n",
      "Epoch 401/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6380 - accuracy: 0.5892 - val_loss: 0.6840 - val_accuracy: 0.5648\n",
      "Epoch 402/500\n",
      "60/60 [==============================] - 5s 84ms/step - loss: 0.6403 - accuracy: 0.5890 - val_loss: 0.6732 - val_accuracy: 0.5797\n",
      "Epoch 403/500\n",
      "60/60 [==============================] - 4s 73ms/step - loss: 0.6393 - accuracy: 0.5850 - val_loss: 0.6765 - val_accuracy: 0.5648\n",
      "Epoch 404/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6379 - accuracy: 0.5934 - val_loss: 0.6826 - val_accuracy: 0.5581\n",
      "Epoch 405/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6392 - accuracy: 0.5892 - val_loss: 0.6862 - val_accuracy: 0.5515\n",
      "Epoch 406/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6364 - accuracy: 0.5908 - val_loss: 0.6879 - val_accuracy: 0.5515\n",
      "Epoch 407/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6381 - accuracy: 0.5853 - val_loss: 0.6807 - val_accuracy: 0.5598\n",
      "Epoch 408/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6376 - accuracy: 0.5879 - val_loss: 0.6730 - val_accuracy: 0.5648\n",
      "Epoch 409/500\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.6367 - accuracy: 0.5984 - val_loss: 0.6691 - val_accuracy: 0.5631\n",
      "Epoch 410/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6387 - accuracy: 0.5916 - val_loss: 0.6744 - val_accuracy: 0.5664\n",
      "Epoch 411/500\n",
      "60/60 [==============================] - 4s 62ms/step - loss: 0.6371 - accuracy: 0.5890 - val_loss: 0.6948 - val_accuracy: 0.5382\n",
      "Epoch 412/500\n",
      "60/60 [==============================] - 4s 72ms/step - loss: 0.6418 - accuracy: 0.5864 - val_loss: 0.6804 - val_accuracy: 0.5681\n",
      "Epoch 413/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6379 - accuracy: 0.5942 - val_loss: 0.6870 - val_accuracy: 0.5681\n",
      "Epoch 414/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6383 - accuracy: 0.5898 - val_loss: 0.6807 - val_accuracy: 0.5615\n",
      "Epoch 415/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6383 - accuracy: 0.5890 - val_loss: 0.6773 - val_accuracy: 0.5615\n",
      "Epoch 416/500\n",
      "60/60 [==============================] - 4s 62ms/step - loss: 0.6402 - accuracy: 0.5850 - val_loss: 0.6764 - val_accuracy: 0.5648\n",
      "Epoch 417/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6372 - accuracy: 0.5900 - val_loss: 0.6741 - val_accuracy: 0.5681\n",
      "Epoch 418/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6364 - accuracy: 0.5945 - val_loss: 0.6852 - val_accuracy: 0.5648\n",
      "Epoch 419/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6396 - accuracy: 0.5795 - val_loss: 0.6659 - val_accuracy: 0.6063\n",
      "Epoch 420/500\n",
      "60/60 [==============================] - 5s 76ms/step - loss: 0.6377 - accuracy: 0.5866 - val_loss: 0.6790 - val_accuracy: 0.5698\n",
      "Epoch 421/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6383 - accuracy: 0.5908 - val_loss: 0.6809 - val_accuracy: 0.5681\n",
      "Epoch 422/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6376 - accuracy: 0.5932 - val_loss: 0.6859 - val_accuracy: 0.5432\n",
      "Epoch 423/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6373 - accuracy: 0.5927 - val_loss: 0.6954 - val_accuracy: 0.5449\n",
      "Epoch 424/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6376 - accuracy: 0.5877 - val_loss: 0.6895 - val_accuracy: 0.5548\n",
      "Epoch 425/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6394 - accuracy: 0.5840 - val_loss: 0.6932 - val_accuracy: 0.5548\n",
      "Epoch 426/500\n",
      "60/60 [==============================] - 4s 72ms/step - loss: 0.6393 - accuracy: 0.5892 - val_loss: 0.6730 - val_accuracy: 0.5731\n",
      "Epoch 427/500\n",
      "60/60 [==============================] - 4s 70ms/step - loss: 0.6403 - accuracy: 0.5869 - val_loss: 0.6712 - val_accuracy: 0.5814\n",
      "Epoch 428/500\n",
      "60/60 [==============================] - 4s 72ms/step - loss: 0.6376 - accuracy: 0.5945 - val_loss: 0.6806 - val_accuracy: 0.5681\n",
      "Epoch 429/500\n",
      "60/60 [==============================] - 5s 75ms/step - loss: 0.6405 - accuracy: 0.5808 - val_loss: 0.6730 - val_accuracy: 0.5831\n",
      "Epoch 430/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6353 - accuracy: 0.5966 - val_loss: 0.6841 - val_accuracy: 0.5681\n",
      "Epoch 431/500\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.6366 - accuracy: 0.5945 - val_loss: 0.6751 - val_accuracy: 0.5748\n",
      "Epoch 432/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6383 - accuracy: 0.5929 - val_loss: 0.6643 - val_accuracy: 0.5930\n",
      "Epoch 433/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6379 - accuracy: 0.5963 - val_loss: 0.6694 - val_accuracy: 0.5864\n",
      "Epoch 434/500\n",
      "60/60 [==============================] - 4s 73ms/step - loss: 0.6404 - accuracy: 0.5843 - val_loss: 0.6744 - val_accuracy: 0.5797\n",
      "Epoch 435/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6389 - accuracy: 0.5990 - val_loss: 0.6879 - val_accuracy: 0.5631\n",
      "Epoch 436/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6385 - accuracy: 0.5950 - val_loss: 0.6706 - val_accuracy: 0.5831\n",
      "Epoch 437/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6329 - accuracy: 0.6031 - val_loss: 0.6763 - val_accuracy: 0.5764\n",
      "Epoch 438/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6335 - accuracy: 0.5969 - val_loss: 0.6847 - val_accuracy: 0.5731\n",
      "Epoch 439/500\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.6365 - accuracy: 0.5853 - val_loss: 0.6849 - val_accuracy: 0.5814\n",
      "Epoch 440/500\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.6346 - accuracy: 0.5984 - val_loss: 0.6704 - val_accuracy: 0.5980\n",
      "Epoch 441/500\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.6384 - accuracy: 0.5934 - val_loss: 0.6826 - val_accuracy: 0.5764\n",
      "Epoch 442/500\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 0.6368 - accuracy: 0.5958 - val_loss: 0.6658 - val_accuracy: 0.6030\n",
      "Epoch 443/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6380 - accuracy: 0.5950 - val_loss: 0.6874 - val_accuracy: 0.5714\n",
      "Epoch 444/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6369 - accuracy: 0.5929 - val_loss: 0.6677 - val_accuracy: 0.6047\n",
      "Epoch 445/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6371 - accuracy: 0.5919 - val_loss: 0.6904 - val_accuracy: 0.5664\n",
      "Epoch 446/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6366 - accuracy: 0.5974 - val_loss: 0.6978 - val_accuracy: 0.5615\n",
      "Epoch 447/500\n",
      "60/60 [==============================] - 4s 71ms/step - loss: 0.6348 - accuracy: 0.6021 - val_loss: 0.6911 - val_accuracy: 0.5598\n",
      "Epoch 448/500\n",
      "60/60 [==============================] - 4s 73ms/step - loss: 0.6412 - accuracy: 0.5866 - val_loss: 0.7041 - val_accuracy: 0.5565\n",
      "Epoch 449/500\n",
      "60/60 [==============================] - 4s 75ms/step - loss: 0.6374 - accuracy: 0.5903 - val_loss: 0.6855 - val_accuracy: 0.5714\n",
      "Epoch 450/500\n",
      "60/60 [==============================] - 4s 74ms/step - loss: 0.6374 - accuracy: 0.5895 - val_loss: 0.6822 - val_accuracy: 0.5731\n",
      "Epoch 451/500\n",
      "60/60 [==============================] - 5s 76ms/step - loss: 0.6369 - accuracy: 0.5953 - val_loss: 0.6736 - val_accuracy: 0.5781\n",
      "Epoch 452/500\n",
      "60/60 [==============================] - 5s 75ms/step - loss: 0.6349 - accuracy: 0.5958 - val_loss: 0.6779 - val_accuracy: 0.5648\n",
      "Epoch 453/500\n",
      "60/60 [==============================] - 4s 69ms/step - loss: 0.6358 - accuracy: 0.5871 - val_loss: 0.6881 - val_accuracy: 0.5664\n",
      "Epoch 454/500\n",
      "60/60 [==============================] - 5s 76ms/step - loss: 0.6366 - accuracy: 0.5974 - val_loss: 0.6837 - val_accuracy: 0.5681\n",
      "Epoch 455/500\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.6385 - accuracy: 0.5927 - val_loss: 0.6830 - val_accuracy: 0.5714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 456/500\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 0.6336 - accuracy: 0.5955 - val_loss: 0.6849 - val_accuracy: 0.5648\n",
      "Epoch 457/500\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.6394 - accuracy: 0.5874 - val_loss: 0.6647 - val_accuracy: 0.5864\n",
      "Epoch 458/500\n",
      "60/60 [==============================] - 4s 71ms/step - loss: 0.6350 - accuracy: 0.5940 - val_loss: 0.6875 - val_accuracy: 0.5449\n",
      "Epoch 459/500\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.6383 - accuracy: 0.5856 - val_loss: 0.6746 - val_accuracy: 0.5681\n",
      "Epoch 460/500\n",
      "60/60 [==============================] - 4s 62ms/step - loss: 0.6392 - accuracy: 0.5837 - val_loss: 0.6621 - val_accuracy: 0.6030\n",
      "Epoch 461/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6356 - accuracy: 0.6045 - val_loss: 0.6666 - val_accuracy: 0.5764\n",
      "Epoch 462/500\n",
      "60/60 [==============================] - 5s 83ms/step - loss: 0.6390 - accuracy: 0.5874 - val_loss: 0.6686 - val_accuracy: 0.5565\n",
      "Epoch 463/500\n",
      "60/60 [==============================] - 4s 73ms/step - loss: 0.6379 - accuracy: 0.5903 - val_loss: 0.6822 - val_accuracy: 0.5532\n",
      "Epoch 464/500\n",
      "60/60 [==============================] - 4s 62ms/step - loss: 0.6407 - accuracy: 0.5864 - val_loss: 0.6816 - val_accuracy: 0.5648\n",
      "Epoch 465/500\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.6347 - accuracy: 0.5982 - val_loss: 0.7020 - val_accuracy: 0.5332\n",
      "Epoch 466/500\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 0.6397 - accuracy: 0.5924 - val_loss: 0.7006 - val_accuracy: 0.5382\n",
      "Epoch 467/500\n",
      "29/60 [=============>................] - ETA: 2s - loss: 0.6391 - accuracy: 0.5921"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-62af5f47ef3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbalanced_x_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbalanced_y_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    853\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     if (self._delta_t_batch > 0. and\n\u001b[1;32m    303\u001b[0m         delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1):\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m   3493\u001b[0m     \"\"\"\n\u001b[1;32m   3494\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[0;32m-> 3495\u001b[0;31m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[1;32m   3496\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3497\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   3401\u001b[0m         \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3403\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3404\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_median\u001b[0;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[1;32m   3541\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3542\u001b[0m         \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3543\u001b[0;31m     \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3545\u001b[0m     \u001b[0;31m# Check if the array contains any nan's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 2s 14ms/step - loss: 0.6192 - accuracy: 0.6108\n",
      "Test loss: 0.6192388534545898\n",
      "Test accuracy: 0.6107611656188965\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(balanced_x_train, balanced_y_train, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'balanced_x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9dbc5bffcc2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbalanced_x_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'balanced_x_train' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
