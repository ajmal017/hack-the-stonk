{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas_datareader\\compat\\__init__.py:7: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  from pandas.util.testing import assert_frame_equal\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"c:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"c:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"c:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"c:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\n  File \"c:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\imp.py\", line 343, in load_dynamic\n    return _load(spec)\nImportError: DLL load failed: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[0m_mod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\imp.py\u001b[0m in \u001b[0;36mload_module\u001b[1;34m(name, file, filename, details)\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[1;34m(name, path, file)\u001b[0m\n\u001b[0;32m    342\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[1;32m--> 343\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: The specified module could not be found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9ff0a68f0fd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;31m# Protocol buffers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[1;32m---> 69\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"c:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"c:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"c:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"c:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\n  File \"c:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\imp.py\", line 343, in load_dynamic\n    return _load(spec)\nImportError: DLL load failed: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "from pandas_datareader import data as pdr \n",
    "from datetime import date\n",
    "import yfinance as yf \n",
    "yf.pdr_override()\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "import quandl\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_sp = '^GSPC'\n",
    "ticker_gold = 'GC=F'\n",
    "ticker_oil = 'CL=F'\n",
    "ticker_dax = '^GDAXI'\n",
    "ticker_nikkei = '^N225'\n",
    "ticker_ftse = '^FTSE'\n",
    "ticker_shanghai = '000001.SS'\n",
    "\n",
    "auth_tok = \"Nv1rJgRR7u88iz_dg7Y6\"\n",
    "\n",
    "end_date = \"2020-07-1\"\n",
    "start_date = \"1970-01-02\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGOLDData ():\n",
    "    # Contains only price from 1975 onwards\n",
    "    data = quandl.get(\"CHRIS/CME_GC1\", trim_start = start_date, trim_end = end_date, authtoken=auth_tok)\n",
    "    data = data[['Last']]\n",
    "    data.columns = [\"GOLD Adj Close\"]\n",
    "    return data\n",
    "\n",
    "def getSPData():\n",
    "    # Contains price from 1970 onwards\n",
    "    data = pdr.get_data_yahoo(ticker_sp, start=start_date, end=end_date)\n",
    "    data = data[data.columns[4:5]] \n",
    "    data.columns = [\"SP500 Adj Close\"]\n",
    "    return data\n",
    "\n",
    "def getDAXData():\n",
    "    # Contains price from 1988 onwards\n",
    "    data = pdr.get_data_yahoo(ticker_dax, start=start_date, end=end_date)\n",
    "    data = data[data.columns[4:5]]\n",
    "    data.columns = [\"DAX Adj Close\"]\n",
    "    return data\n",
    "\n",
    "\n",
    "def getOILData():\n",
    "    # Contains only price FROM 1984 onwards\n",
    "    data = quandl.get(\"CHRIS/CME_CL1\", trim_start = start_date, trim_end = end_date, authtoken=auth_tok)\n",
    "    data = data[[\"Last\"]]\n",
    "    data.columns=[\"OIL Adj Close\"]\n",
    "    return data\n",
    "\n",
    "\n",
    "def getNIKKEIData():\n",
    "    # Contains only price from 1970 onwards\n",
    "    data = pdr.get_data_yahoo(ticker_nikkei, start=start_date, end=end_date)\n",
    "    data = data[data.columns[4:5]] \n",
    "    data.columns = [\"NIKKEI Adj Close\"]\n",
    "    return data\n",
    "\n",
    "\n",
    "def getFTSEData():\n",
    "    # Contains price from 1984 onwards\n",
    "    data = pdr.get_data_yahoo(ticker_ftse, start=start_date, end=end_date)\n",
    "    data = data[data.columns[4:5]] \n",
    "    data.columns = [\"FTSE Adj Close\"]\n",
    "    return data\n",
    "\n",
    "def getSHANGHAIData():\n",
    "    # Contains only price from 1997\n",
    "    data = pdr.get_data_yahoo(ticker_shanghai, start=start_date, end=end_date)\n",
    "    data = data[data.columns[4:5]] \n",
    "    data.columns = [\"SHANGHAI Adj Close\"]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineData():\n",
    "    allData = [getSPData(), getDAXData(), getFTSEData(), getGOLDData(), getOILData()]\n",
    "    mergedData = pd.concat(allData, axis = 1)\n",
    "    cleanData = mergedData.dropna()\n",
    "    return cleanData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = combineData()\n",
    "\n",
    "FUTURE_TO_PREDICT = 1\n",
    "\n",
    "data['Future'] = data['SP500 Adj Close'].shift(-FUTURE_TO_PREDICT)\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "def buy_or_sell (current, future):\n",
    "    if (future > current):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "data['Target'] = list(map(buy_or_sell, data['SP500 Adj Close'], data['Future']))\n",
    "\n",
    "NUMBER_OF_DATA_POINTS = len(data)\n",
    "SIZE_TRAINING = int(NUMBER_OF_DATA_POINTS * 0.85)\n",
    "SIZE_TESTING  = NUMBER_OF_DATA_POINTS - SIZE_TRAINING\n",
    "print(\"size of training data: {}\".format(SIZE_TRAINING))\n",
    "print(\"size of testing data: {}\".format(SIZE_TESTING))\n",
    "\n",
    "data_training = data[:SIZE_TRAINING]\n",
    "data_testing  = data[SIZE_TRAINING:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            SP500 Adj Close  DAX Adj Close  FTSE Adj Close  GOLD Adj Close  \\\n",
      "Date                                                                         \n",
      "1987-12-30       247.860001    1005.190002     1759.800049           485.5   \n",
      "1988-01-04       255.940002     956.489990     1713.900024           480.5   \n",
      "1988-01-05       258.630005     996.099976     1789.599976           483.2   \n",
      "1988-01-06       258.890015    1006.010010     1787.099976           485.3   \n",
      "1988-01-07       261.070007    1014.469971     1787.199951           483.1   \n",
      "...                     ...            ...             ...             ...   \n",
      "2015-06-19      2109.989990   11040.099609     6710.500000          1201.5   \n",
      "2015-06-22      2122.850098   11460.500000     6825.700195          1183.7   \n",
      "2015-06-23      2124.199951   11542.540039     6834.899902          1176.2   \n",
      "2015-06-24      2108.580078   11471.259766     6844.799805          1172.6   \n",
      "2015-06-25      2102.310059   11473.129883     6807.799805          1171.5   \n",
      "\n",
      "            OIL Adj Close       Future  Target  \n",
      "Date                                            \n",
      "1987-12-30          16.89   255.940002       1  \n",
      "1988-01-04          17.69   258.630005       1  \n",
      "1988-01-05          17.85   258.890015       1  \n",
      "1988-01-06          17.82   261.070007       1  \n",
      "1988-01-07          17.39   243.399994       0  \n",
      "...                   ...          ...     ...  \n",
      "2015-06-19          59.61  2122.850098       1  \n",
      "2015-06-22          59.20  2124.199951       1  \n",
      "2015-06-23          61.01  2108.580078       0  \n",
      "2015-06-24          60.27  2102.310059       0  \n",
      "2015-06-25          59.70  2101.489990       0  \n",
      "\n",
      "[6677 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    data.drop(\"Future\", axis = 1, inplace = True)  # Drop the future column so that the NN doesn't have access to the future\n",
    "    \n",
    "    for column in data.columns:  # Normalize the columns\n",
    "        if column != \"Target\":   # We only want to normalize the other columns\n",
    "            data[column] = data[column].pct_change()   # Normalization by percent change\n",
    "            #data.dropna(inplace = True)\n",
    "            #data[column] = preprocessing.scale(data[column].values)    # First testing without scaling\n",
    "    \n",
    "    data.dropna(inplace = True)\n",
    "\n",
    "    sequential_data = []\n",
    "    sequence_length = 15\n",
    "    \n",
    "    for day in range (len(data)-sequence_length+1):\n",
    "        \n",
    "        sequence = []\n",
    "        \n",
    "        for future_day in range (sequence_length):\n",
    "            sequence.append(data.iloc[day + future_day][:-1])\n",
    "        \n",
    "        buy_or_sell = data.iloc[day + sequence_length - 1][-1]\n",
    "        \n",
    "        sequential_data.append([sequence, buy_or_sell])\n",
    "    \n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    buy_sequences  = []\n",
    "    sell_sequences = []\n",
    "    \n",
    "    for sequence, target in sequential_data:\n",
    "        if target == 1:\n",
    "            buy_sequences.append([sequence, target])\n",
    "        elif target == 0:\n",
    "            sell_sequences.append([sequence, target])\n",
    "            \n",
    "    print(\"{} buys\".format(len(buy_sequences)))\n",
    "    print(\"{} sells\".format(len(sell_sequences)))\n",
    "    \n",
    "    random.shuffle(buy_sequences)\n",
    "    random.shuffle(sell_sequences)\n",
    "    \n",
    "    max_size = min(len(buy_sequences), len(sell_sequences))\n",
    "    \n",
    "    print(\"reduced to {} buys and sells\".format(max_size))\n",
    "    \n",
    "    buy_sequences  = buy_sequences[:max_size]\n",
    "    sell_sequences = sell_sequences[:max_size]\n",
    "    \n",
    "    sequential_data = buy_sequences + sell_sequences\n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for sequence, target in sequential_data:\n",
    "        x.append(sequence)\n",
    "        y.append(target)\n",
    "        \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "c:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "c:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3577 buys\n",
      "3085 sells\n",
      "reduced to 3085 buys and sells\n",
      "634 buys\n",
      "530 sells\n",
      "reduced to 530 buys and sells\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = process_data(data_training)\n",
    "x_test, y_test   = process_data(data_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(128, activation = 'relu', input_shape = (x_train.shape[1:]), return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128, return_sequences = True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay = 1e-6)\n",
    "\n",
    "model.compile(\n",
    "    loss = 'sparse_categorical_crossentropy',\n",
    "    optimizer = opt,\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "NAME = \"first_trial\"\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'numpy.float64'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-22f1f63b5ca8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;32mc:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    813\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    814\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 815\u001b[1;33m           model=self)\n\u001b[0m\u001b[0;32m    816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[0;32m   1097\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m     self._adapter = adapter_cls(\n\u001b[0;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\juand\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    961\u001b[0m         \u001b[1;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m         \"input: {}, {}\".format(\n\u001b[1;32m--> 963\u001b[1;33m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[0;32m    964\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'numpy.float64'>\"})"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size = 32,\n",
    "    epochs = 20,\n",
    "    validation_data = (x_test, y_test),\n",
    "    callbacks=[tensorboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train))\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
